<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js rust">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "rust";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">Introduction</a></li><li class="chapter-item expanded affix "><a href="CONTRIBUTING.html">Contributing to the Guide</a></li><li class="chapter-item expanded affix "><li class="part-title">General Fansubbing</li><li class="chapter-item expanded "><a href="overview/preface.html"><strong aria-hidden="true">1.</strong> Preface</a></li><li class="chapter-item expanded "><a href="overview/roles.html"><strong aria-hidden="true">2.</strong> Roles</a></li><li class="chapter-item expanded "><a href="overview/requirements.html"><strong aria-hidden="true">3.</strong> Requirements</a></li><li class="chapter-item expanded affix "><li class="part-title">Encoding</li><li class="chapter-item expanded "><a href="encoding/preparation.html"><strong aria-hidden="true">4.</strong> Preparations and Necessary Software</a></li><li class="chapter-item expanded "><a href="encoding/basics-and-workflow.html"><strong aria-hidden="true">5.</strong> Basics and General Workflow</a></li><li class="chapter-item expanded "><a href="encoding/video-artifacts.html"><strong aria-hidden="true">6.</strong> Recognizing Video Artifacts</a></li><li class="chapter-item expanded "><a href="encoding/scenefiltering.html"><strong aria-hidden="true">7.</strong> Scenefiltering</a></li><li class="chapter-item expanded "><a href="encoding/masking-limiting-etc.html"><strong aria-hidden="true">8.</strong> Masking, Limiting, and Related Functions</a></li><li class="chapter-item expanded "><a href="encoding/descaling.html"><strong aria-hidden="true">9.</strong> Descaling</a></li><li class="chapter-item expanded "><a href="encoding/resampling.html"><strong aria-hidden="true">10.</strong> Resampling</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">11.</strong> Codecs</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="encoding/codecs/x264.html"><strong aria-hidden="true">11.1.</strong> x264</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Typesetting</li><li class="chapter-item expanded "><a href="typesetting/aegisub.html"><strong aria-hidden="true">12.</strong> Aegisub and Other Tools</a></li><li class="chapter-item expanded affix "><li class="part-title">Resources and References</li><li class="chapter-item expanded "><a href="archived-websites/bt601-vs-bt709.html"><strong aria-hidden="true">13.</strong> Colorspaces</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><a href="impressum.html">Impressum</a></li><li class="chapter-item expanded affix "><a href="privacy-policy.html">Privacy Policy</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        <a href="https://github.com/Irrational-Encoding-Wizardry/guide.encode.moe" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#fansubbing-guide" id="fansubbing-guide">Fansubbing Guide</a></h1>
<p>This project aims to become a public guide
for aspiring as well as veteran fansubbers
that everyone can contribute to.
It is currently under construction
as many topics are still missing,
but will hopefully cover all important areas
and roles that go into the fansubbing process.
(<em>Please help by improving the pages
and filling in TODO entries.
More details are provided on the next page.</em>)</p>
<p>To start reading,
you can click on the arrow on the right
or press your <kbd>→</kbd> key
to go to the next page,
or you can browse the page index on the left.</p>
<p>The project is <a href="https://github.com/Irrational-Encoding-Wizardry/guide.encode.moe/">hosted on Github</a>
and available to read online at <a href="https://guide.encode.moe/">https://guide.encode.moe/</a>.</p>
<h1><a class="header" href="#contribution-guidelines" id="contribution-guidelines">Contribution Guidelines</a></h1>
<p>If you are interested in supporting
or contributing to this guide,
please keep on reading.</p>
<p><strong>If you are not,
feel free to skip to the next page.</strong></p>
<hr />
<p>We are currently still in the early phases of this guide,
so any form of contribution,
including just giving feedback,
is greatly appreciated.
Please open an issue on our <a href="https://github.com/Irrational-Encoding-Wizardry/guide.encode.moe/issues">Github repository</a>
with your feedback,
or begin working on a Pull Request.</p>
<p>If you are mainly looking for things to work on,
refer to the <a href="CONTRIBUTING.html#todo">TODO</a> section.</p>
<h2><a class="header" href="#general" id="general">General</a></h2>
<h3><a class="header" href="#language" id="language">Language</a></h3>
<p>The language of this guide is <strong>English</strong>.
American or British English are both acceptable
and there is no preference for either.</p>
<p>The only exceptions are
pages specific to a particular language,
for example with references to online dictionaries
or official grammar rule books,
or other typographic advices,
for example concerning the usage of quotation marks.</p>
<p>When adding such a page,
please briefly describe in your Pull Request
what the text is about,
what topics it covers,
and, if necessary,
why it only applies to a specific language.</p>
<h3><a class="header" href="#technology" id="technology">Technology</a></h3>
<p>This guide is written in <a href="https://en.wikipedia.org/wiki/Markdown">Markdown</a> and uses <a href="https://github.com/rust-lang/mdBook">Rust’s mdBook</a> to compile
the static HTML pages.</p>
<p>In order to build and preview the guide locally,
you only need to <a href="https://github.com/rust-lang/mdBook/tree/a00e7d17695d43af1f7999008b08a75bcb0c134f#installation">install mdBook</a>,
which can be done via the provided binaries
or directly installing via <a href="https://crates.io/">Crates.io</a>, Rust’s package registry:</p>
<pre><code>$ cargo install mdbook
Updating crates.io index
Installing mdbook v0.4.1
Downloaded syn v1.0.38
...
Downloaded 4 crates (501.9 KB) in 0.42s
Compiling libc v0.2.74
...
Compiling mdbook v0.4.1
Finished release [optimized] target(s) in 2m 56s
</code></pre>
<p>Once an <code>mdbook</code> executable is installed,
running <code>mdbook serve</code> in the root directory of the guide’s repository
and opening <code>http://localhost:3000</code> with your browser
will show a preview of the book.
Any changes you make to the source <code>.md</code> files
will cause your browser to be refreshed and automatically reloaded.</p>
<pre><code>$ mdbook serve
[INFO] (mdbook::book): Book building has started
[INFO] (mdbook::book): Running the html backend
[INFO] (mdbook::cmd::serve): Serving on: http://localhost:3000
[INFO] (warp::server): Server::run; addr=V6([::1]:3000)
[INFO] (warp::server): listening on http://[::1]:3000 
[INFO] (mdbook::cmd::watch): Listening for changes...
</code></pre>
<p>Changes to the theme can be done by editing the <code>.css</code> files in <code>/theme/css/</code>.
For information on adding plug-ins or changing the way the book is built,
see the <a href="https://rust-lang.github.io/mdBook/">mdBook User Guide</a>.</p>
<h3><a class="header" href="#adding-a-new-page" id="adding-a-new-page">Adding a New Page</a></h3>
<p>In order for your page to be accessible,
you need to add it to the <code>SUMMARY.md</code> file.
The title used there will be used in the navigation bar,
so keep it short.</p>
<h3><a class="header" href="#todo" id="todo">TODO</a></h3>
<p>Various sections are still under construction.
You will occasionally find <code>TODO</code> as verbatim text
or within comments.</p>
<p>Our goal is to have a section
with one or more pages
for each of the roles
specified in the roles page.</p>
<p>Feel free to work on any of the <code>TODO</code> marks
or create a new section.</p>
<p>Currently, we aim to add the following topics
in no particular priority:</p>
<ul>
<li>Workflow</li>
<li>Translation</li>
<li>Edit</li>
<li>Timing
<ul>
<li>Basic Procedure</li>
<li>Snapping</li>
<li>Joining, Splitting</li>
<li>Post-processing (TPP &amp; Useful Scripts)</li>
<li>Shifting &amp; <a href="https://github.com/tp7/Sushi/releases">Sushi</a></li>
<li>Karaoke</li>
</ul>
</li>
<li>Typesetting
<ul>
<li>…with Aegisub
<ul>
<li>Styling (of dialogue)</li>
<li>Signs
<ul>
<li>Positioning, Layers, Rotation, Perspective, …</li>
</ul>
</li>
<li>Masking</li>
<li><a href="https://github.com/TypesettingTools">Automation Scripts</a></li>
<li>Movement &amp; Motion Tracking</li>
</ul>
</li>
<li>…<a href="https://typesettingtools.github.io/2014/08/25/typesetting-with-illustrator-and-ai2ass-part-1.html">with Adobe Illustrator</a></li>
<li>(…with Adobe After Effects)</li>
</ul>
</li>
<li>Encoding [<em>I’m sure there’s something to be done</em>]</li>
<li>Quality Check</li>
<li>Karaoke Effects</li>
</ul>
<p>There is a collection of links <a href="https://github.com/Irrational-Encoding-Wizardry/guide.encode.moe/issues/2">here</a> that can be used as reference
when working on any future section.</p>
<h2><a class="header" href="#style-guidelines" id="style-guidelines">Style Guidelines</a></h2>
<p>The following are the style guidelines
for various aspects of this guide.
The most important aspect is having <strong>Semantic Linefeeds</strong>.
The other points may serve as guidelines for formatting future pages.
Refer to the <a href="https://www.markdownguide.org/basic-syntax">Markdown Guide</a>
for guidelines on visual formatting.</p>
<h3><a class="header" href="#semantic-linefeeds-" id="semantic-linefeeds-">Semantic Linefeeds (!)</a></h3>
<p>Always use <a href="https://rhodesmill.org/brandon/2012/one-sentence-per-line/">Semantic Linefeeds</a> when editing text.
They are used
to break lines into logical units
rather than after a certain line length threshold is reached!</p>
<p>They drastically improve sentence parsing
in the human brain
and make code diffing much more simple
compared to hard-wrapping at 80 columns.
You should still aim
not to exceed 80 columns in a single line,
but unless you are writing code or URLs,
you will most likely not have any problems with this.
Markdown will collapse adjacent lines into a paragraph,
so you don’t have to worry about the rendered result.</p>
<p>As a rule of thumb,
always start a new line on
a comma,
a period,
any other sentence terminating punctuation,
parenthesized sentences (not words),
or new items in a long list
(such as the one you are reading right now).</p>
<h3><a class="header" href="#indentation" id="indentation">Indentation</a></h3>
<p>The indent size is <strong>two spaces</strong>.</p>
<h3><a class="header" href="#lists" id="lists">Lists</a></h3>
<p>Unordered list lines should be indented <strong>once</strong>,
while ordered lists are indented <strong>twice</strong>.
The text of an unordered item should have one space after the <code>-</code>,
while the text of an ordered item
should start on the fourth column
(start every line with the number 1).</p>
<pre><code class="language-md">- This is an unordered list
  - With a sublist
  - And another item in that sublist
</code></pre>
<pre><code class="language-md">1. This is an ordered list
1. Another list item
…
1. Last entry of the list
</code></pre>
<h3><a class="header" href="#blank-lines" id="blank-lines">Blank Lines</a></h3>
<p>All block lists
should be separated from text
with a blank line on each side.
The same applies to code blocks.</p>
<p>Separate headings from text with <strong>two</strong> blank lines before the heading,
and <strong>one</strong> after.
Headings immediately following their parent heading
only need one blank line in-between.</p>
<p>Additionally, separate text from end-of-section hyperlink lists
with <strong>one</strong> blank line before the list.
For image embeds,
there should be a blank line on each side.</p>
<p>Horizontal rules can be useful for splitting subsections or
as a visual guide to where the next explanation begins.
They are created with a sole <code>---</code> on its own line,
and must have a blank line on each side.</p>
<h3><a class="header" href="#hyperlinking" id="hyperlinking">Hyperlinking</a></h3>
<p>There are three types of hyperlinks.</p>
<ul>
<li>The text you want highlighted is more than one word,
or different than the shorthand name of the link.
<ul>
<li><code>[website's great article][short]</code></li>
<li><a href="https://guide.encode.moe/">website’s great article</a></li>
</ul>
</li>
<li>The text you want highlighted is the same as the shorthand.
<ul>
<li><code>[short][]</code></li>
<li><a href="https://guide.encode.moe/">short</a></li>
</ul>
</li>
<li>You want the full address displayed.
<ul>
<li><code>&lt;https://guide.encode.moe/&gt;</code></li>
<li><a href="https://guide.encode.moe/">https://guide.encode.moe/</a></li>
</ul>
</li>
</ul>
<p>For the first two hyperlinking styles,
you will want to include a line at the end of that header section
in the following format.</p>
<p><code>[short]: https://guide.encode.moe/</code></p>
<p>If there are multiple links used in the first two styles,
you will want multiple lines at the end of the header section.</p>
<pre><code class="language-md">[short1]: https://guide.encode.moe/
[short2]: https://guide.encode.moe/CONTRIBUTING.HTML
…
</code></pre>
<p>For relative links (links to other pages,
images,
or files within this repository),
follow the guidelines for <a href="https://github.com/benbalter/jekyll-relative-links/blob/master/README.md">Jekyll Relative Links</a>.</p>
<h4><a class="header" href="#section-linking" id="section-linking">Section Linking</a></h4>
<p>If you are linking to a section on the same page,
<code>[section name](#header)</code> is allowed in-line.
An example of this is <a href="CONTRIBUTING.html#hyperlinking">the hyperlink section you are reading</a>.
In markdown, this is simply <code>[the hyperlink section you are reading](#hyperlinking)</code>.</p>
<p>Section names are converted to all lowercase,
replacing spaces with a <code>-</code> dash,
while disregarding all non-alphanumeric characters
with the exception of the literal <code>-</code> dash being kept.
Therefore, a section named <code>$aFoo-Bar b2 !</code> can be referenced
as <code>foobar.md#afoo-bar-b2-</code>.</p>
<h3><a class="header" href="#adding-images" id="adding-images">Adding Images</a></h3>
<p>When adding images to your paragraphs,
use the following syntax<sup class="footnote-reference"><a href="#1">1</a></sup>:</p>
<pre><code>![](images/filename.png)
*Visible caption text*
</code></pre>
<p>Make sure your image is separated from other images or text
with a blank line above and below,
as this will align them correctly
and allow for the caption to be displayed.</p>
<div class="warning box"><p>
Try to avoid adding lossy images to the guide
(all screenshots should be lossless from the source).
Also, make sure your image is compressed as much as possible
<strong>before committing</strong> it.
This can be done with <a href="https://www.css-ig.net/pingo" target="_blank">pingo</a>'s
lossless PNG compression: <code>pingo -sa file.png</code>.
</p></div>
<p>When extracting frames directly from a VapourSynth pipline
where the format might be <code>vs.YUV420P16</code> (YUV 4:2:0, 16-bit),
convert your image to <code>vs.RGB24</code> (RGB 8-bit) before saving as a PNG.
This is because many, if not all, browsers don’t support
images with bit-depths higher than 8 bpp,
and the dithering behavior of some browsers may be different from others
or poorly executed.</p>
<p>You can change the format and bit-depth
while saving to a PNG file with the following lines:</p>
<pre><code class="language-py"># replace `{frame}` with the frame number of the clip you are extracting
out = core.imwri.Write(clip[{frame}].resize.Bicubic(format=vs.RGB24, matrix_in_s='709', dither_type='error_diffusion', filter_param_a_uv=0.33, filter_param_b_uv=0.33), 'PNG', '%06d.png', firstnum={frame})
out.get_frame(0)
</code></pre>
<h3><a class="header" href="#citations" id="citations">Citations</a></h3>
<p>If you are archiving another website’s text
or copying their images into this repository,
make sure to cite your sources using APA formatting.
To generate APA citations,
use <a href="https://papersowl.com/apa-citation-generator">PapersOwl</a>.
Only use this if you fear the website is not a permanent source.</p>
<p>For mid-document citations,
use “in-text citations” with footnotes for the full citations.
For a full document citation,
simply place the full citation at the bottom of the document,
under a horizontal rule.</p>
<h3><a class="header" href="#footnotes" id="footnotes">Footnotes</a></h3>
<p>Footnotes can be used for information that would interrupt
the flow or purpose of a paragraph,
but may still be of interest.
They are created with <code>[^#]</code> in-text,
and an additional <code>[^#]: Text here...</code> at the bottom of the page,
separated by a horizontal rule <code>---</code>,
where <code>#</code> is to be replaced with an increasing
and per-page unique number.</p>
<h3><a class="header" href="#infowarning-boxes" id="infowarning-boxes">Info/Warning boxes</a></h3>
<p>Info boxes can be used similarly to footnotes,
but for information that the reader might want to know
before continuing to read the rest of the page.</p>
<p>Warning boxes are similar but are for information
that is necessary for the reader to know
before continuing to read the rest of the page.</p>
<p>The current syntax uses in-line HTML to render these paragraphs with a
different CSS style.
These paragraphs must be separated with a blank line above and below similar
to images or code blocks.</p>
<pre><code class="language-md">&lt;div class=&quot;info box&quot;&gt;&lt;p&gt;
Text here as usual, using semantic linefeed rules.
If you need text-formatting, you &lt;strong&gt;must&lt;/strong&gt; use in-line HTML.
&lt;/p&gt;&lt;/div&gt;
</code></pre>
<pre><code class="language-md">&lt;div class=&quot;warning box&quot;&gt;&lt;p&gt;
This class should be used for important information.
&lt;/p&gt;&lt;/div&gt;
</code></pre>
<h3><a class="header" href="#mathematics-with-mathjax" id="mathematics-with-mathjax">Mathematics with MathJax</a></h3>
<p>This guide has MathJax support,
so in-line or block mathematics can be rendered with TeX.
This obviously requires knowledge of TeX syntax and the supported functions
listed in the <a href="http://docs.mathjax.org/en/latest/input/tex/index.html">MathJax documentation</a>.
To start in-line formulas, the syntax is <code>\\( ... \\)</code>.
On the other hand, the block formulas’ syntax is:</p>
<pre><code class="language-md">$$
...
$$
</code></pre>
<p>Similar to ``` fenced code blocks,
separate these blocks with one blank line on either side.</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>This differs from normal Markdown image syntax,
by abusing CSS tags to render the <code>Visual caption text</code>
centered and under the image.
This may be changed in the future with a plug-in.</p>
</div>
<h1><a class="header" href="#preface" id="preface">Preface</a></h1>
<p><strong>What does it take to be a fansubber?</strong></p>
<p>While I’d like to say that the most and only important thing is
<strong>a healthy (or unhealthy) love of anime</strong>,
that would be a lie.
Being a fansubber takes a lot of work—it can be like having a second job,
depending on how many projects you pick up.
Even though fansubbers provide a free product as volunteers,
there are still expectations that you will put in the time and effort
to complete your part in a timely manner.</p>
<p>Now, I don’t want to scare you away,
but I do want you to be aware of what you’re getting into.</p>
<p>The most successful fansubbers are often those with lots of spare time
such as students,
physically disabled persons,
and single adults without many commitments outside of work.
While an unrestricted schedule isn’t a hard requirement,
it is something to keep in mind as you start this process.</p>
<p>That said, some roles can be a means of keeping up or honing a skill.
The translator, editor, and quality checker roles are particularly
suited to this situation as they require skills that are easily
applicable to careers outside of fansubbing.
However, these roles are also incredibly difficult to teach,
especially with the resources available to fansubbers,
so if they are addressed in this guide,
it will not be in as much depth as other roles.</p>
<p>If you don’t even know what roles there are to choose from,
don’t worry—we’ll get there.
For now, let’s move on to some practical requirements.</p>
<h1><a class="header" href="#roles" id="roles">Roles</a></h1>
<p>There are 8 (sometimes 9) major roles in every fansub group. They are:</p>
<ol>
<li><a href="overview/roles.html#encoder"><strong>Encoder</strong></a></li>
<li><a href="overview/roles.html#timer"><strong>Timer</strong></a></li>
<li><a href="overview/roles.html#typesetter"><strong>Typesetter</strong></a></li>
<li><a href="overview/roles.html#editor"><strong>Editor</strong></a></li>
<li><a href="overview/roles.html#quality-checker"><strong>Quality Checker</strong></a></li>
<li><em>Optional</em>: <a href="overview/roles.html#translator--translation-checker">Translator</a></li>
<li><em>Optional</em>: <a href="overview/roles.html#translator--translation-checker">Translation Checker</a></li>
<li><em>Optional</em>: <a href="overview/roles.html#karaoke-effect-creator">Karaoke Effects Creator</a></li>
<li><em>Optional</em>: Project Leader</li>
</ol>
<p>In this guide, we will only be providing in-depth guides for the
Encoder, Timer, and Typesetter roles.
However, Quality Checkers are often expected to be familiar with
most or all of the roles in order to recognize errors.</p>
<p>This page serves as just an overview of the work
various roles will be expected to complete.</p>
<h2><a class="header" href="#encoder" id="encoder">Encoder</a></h2>
<p>Time commitment per episode: 20 minutes - 2 hours active (4-12 hours
inactive)</p>
<p>Encoders (sometimes abbreviated as <em>ENC</em>) are responsible for the audio
and video.
They will generally be provided with one or more video sources
and are expected to produce the best video possible within reason.</p>
<p>This is done with a <strong>frame-by-frame video processor</strong> such as AviSynth
and VapourSynth<sup class="footnote-reference"><a href="#1">1</a></sup>,
a <strong>video encoder</strong> such as x264 or x265<sup class="footnote-reference"><a href="#2">2</a></sup>,
and <strong>audio tools</strong> such as eac3to,
qaac,
and FLAC<sup class="footnote-reference"><a href="#3">3</a></sup>.
This is not a comprehensive list,
but it does represent the broad categories of tools required.</p>
<p>Encoders are expected to have a high level of skill and understanding of
video concepts and tools.
It is perhaps the most technically challenging role in fansubbing.
However, much of the work is repeatable,
as each episode in a show will usually be very similar to every other one.
It will also get easier over time as they become more familiar with the
concepts and tools.</p>
<p>One last note about encoding: there are as many opinions about how to
fix video problems as there are encoders.
Encoders can and often do become contentious about their work,
theories,
and scripts.
It’s important to keep in mind that a disagreement is not always an insult,
and more experienced encoders often just want to help and provide feedback.
The important part is the result!</p>
<h2><a class="header" href="#timer" id="timer">Timer</a></h2>
<p>Time commitment per episode: 20 minutes - 4 hours</p>
<p>The Timer (abbreviated <em>TM</em>) is responsible for when the text
representing spoken dialogue shows up on screen.</p>
<p>The timing of subtitles is much more important than one might assume.
The entrance and exit times of the subtitles,
or a fluid transition from one line to the next,
can make a large impact on the “watchability” of the episode as a whole.
Take, for example,
the following clip from Eromanga-sensei:</p>
<div style="width:100%;height:0px;position:relative;padding-bottom:28.125%;"><iframe src="https://streamable.com/s/5kylp/abmmlr" frameborder="0" width="100%" height="100%" allowfullscreen style="width:100%;height:100%;position:absolute;left:0px;top:0px;overflow:hidden;"></iframe></div>
<p>On the left are the official subtitles from Amazon’s AnimeStrike,
and on the right is a fansub release.
There are many problems with Amazon’s subtitles:
entering and exiting the screen up to two seconds late,
presenting 4-5 lines on screen at once,
and not separating dialogue based on speaking character.
These problems detract from the viewing experience,
drawing attention to the appearance of the subtitles
and distracting from the story and video.</p>
<h2><a class="header" href="#typesetter" id="typesetter">Typesetter</a></h2>
<p>Time commitment per episode: 20 minutes - 8+ hours (dependent on number
and difficulty of signs)</p>
<p>Typesetters (abbreviated <em>TS</em>) are responsible for the visual
presentation of translated text on-screen. These are generally called <em>signs</em>.</p>
<p>For example, given this scene and a translation of “Adachi Fourth Public High School”…</p>
<p><img src="overview/images/cnvimage100.png" alt="" />
<em>[DameDesuYo] Eromanga-sensei - 01 (1920x1080 10bit AAC) [05CB518E].mkv_snapshot_03.11_[2017.08.18_21.14.55].jpg</em></p>
<p>the Typesetter would be expected to produce something like
this:</p>
<p><img src="overview/images/cnvimage101.png" alt="" />
<em>[DameDesuYo] Eromanga-sensei - 01 (1920x1080 10bit AAC) [05CB518E].mkv_snapshot_03.11_[2017.08.18_21.14.43].jpg</em></p>
<p>Almost every sign the Typesetter works on will be unique,
requiring ingenuity,
a wild imagination,
a sense of style,
and a high degree of attention to detail.
The Typesetter’s goal is to produce something that integrates so
well into the video that the viewer does not realize that it is actually
part of the subtitles.</p>
<p>The sign above is actually one of the more simple kinds that the Typesetter might
have to deal with.
It is <em>static</em>, meaning it does not move,
and has plenty of room around it to place the translation.
Other signs will be much more difficult.
Take for example this scene from Kobayashi-san Chi no
Maid Dragon:</p>
<div style="width:100%;height:0px;position:relative;padding-bottom:28.125%;"><iframe src="https://streamable.com/s/d21iq/aqaodi" frameborder="0" width="100%" height="100%" allowfullscreen style="width:100%;height:100%;position:absolute;left:0px;top:0px;overflow:hidden;"></iframe></div>
<p>Though it may be hard to believe,
the typesetting on the right side of the screen was done entirely
with <em>softsubs</em> (using Aegisub),
subtitles that can be turned on and off in the video player
as compared to <em>hardsubs</em> (using Adobe After Effects) which are burned in.
Each group and language “scene” will have different standards
in regards to soft and hardsubs.
For example, in the English scene,
hardsubs are considered highly distasteful,
whereas in the German scene they are readily accepted.</p>
<p>Something to remember about typesetting is that there is no one way to
typeset a sign.
There are, however,
incorrect ways that are not visually pleasing,
do not match the original well,
are difficult to read,
or are too <em>heavy</em> (meaning computer resource intensive).</p>
<h2><a class="header" href="#editor" id="editor">Editor</a></h2>
<p>Time commitment per episode: 2-4+ hours</p>
<p>The Editor (sometimes abbreviated <em>ED</em>) is responsible for making sure that
the script reads well.
Depending on the source of the script,
this may mean grammatical corrections and some rewording
to address recommendations from the Translation Checker.
However, more often than not,
the job will entail rewriting,
rewording,
and characterizing large portions of the script.
Each group will have different expectations of an Editor
in terms of the type,
style,
and number of changes made.
The Editor may also be responsible
for making corrections recommended by the Quality Checkers.</p>
<h2><a class="header" href="#quality-checker" id="quality-checker">Quality Checker</a></h2>
<p>Time commitment per episode: 30 minutes to 4 hours (depending on your
own standards)</p>
<p>Quality Checkers (abbreviated <em>QC</em>) are often the last eyes on an
episode before it is released.
They are responsible for ensuring that the overall quality
of the release is up to par with the group’s standards.
They are also expected to be familiar with the workflow
and many intricacies of every other role.
Each group has a different approach to how the Quality Checker
completes their work.
For example, one group might require an organized “QC report”
with recommended changes and required fixes,
while other groups may prefer the Quality
Checker to make changes directly to the script whenever possible.</p>
<h2><a class="header" href="#translator--translation-checker" id="translator--translation-checker">Translator &amp; Translation Checker</a></h2>
<p>Time commitment per episode: 1-3 hours for translation check,
4+ hours for an original translation (dependent on the skill of the TL/TLC
and the difficulty of the show’s original script)</p>
<p>The job of the Translator (abbreviated <em>TL</em>) and the Translation Checker
(abbreviated <em>TLC</em>) is to translate and ensure the translational quality
of the script and signs respectively.
This is perhaps an obvious statement,
but it bears explaining just in case.
Today, many shows are <em>simulcast</em> by one or more companies,
meaning scripts will be available either immediately or soon after airing in Japan.
In these cases, some fansub groups may choose to edit
and check the simulcast script rather than translate it from scratch.
This depends almost entirely on the quality of the simulcast.
Fixing a bad simulcast script may be harder than doing an 
<em>original translation</em> (abbreviated <em>OTL</em>).
Finally, translators are responsible for transcribing and translating
opening,
ending,
and insert songs as well.</p>
<h2><a class="header" href="#karaoke-effect-creator" id="karaoke-effect-creator">Karaoke Effect Creator</a></h2>
<p>Time commitment: several hours, once or twice per season</p>
<p>The Karaoke Effect Creator (abbreviated <em>KFX</em>) styles and
adds effects to the lyrics and sometimes romaji and/or kanji for
opening,
ending,
and insert songs.
This can be very similar to typesetting
but utilizes a different set of tools
and can be highly programming-oriented.</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p><em>TODO - sources for AviSynth and VapourSynth builds relevant to fansubbing.</em></p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Further reading on the x264 and x265 libraries can be found <a href="https://www.reddit.com/r/anime/comments/8ktmvu/nerdpost_how_fansubbers_make_your_anime_look/">here</a>.</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>Comparisons of various audio codecs can be found <a href="https://en.wikipedia.org/wiki/Comparison_of_audio_coding_formats">here</a>.</p>
</div>
<h1><a class="header" href="#requirements" id="requirements">Requirements</a></h1>
<h3><a class="header" href="#language-1" id="language-1">Language</a></h3>
<p>There are fansub groups for almost every language in the world.
So, while anyone is welcome to read and use this textbook,
I recommend applying your skills in a fansub group
centered on your native language.
Of course,
there are some roles that don’t require any language skills to complete them.
But you will still need to communicate with the other
members of your chosen fansub group,
and a language barrier can make that difficult.</p>
<h3><a class="header" href="#hardware" id="hardware">Hardware</a></h3>
<p>Every fansubber will need a computer.
Some roles will have higher requirements.
Below are some <strong>minimum <em>recommended</em> computer specifications</strong>
based on the role.
Can you do the job with less than what’s below?
Probably, but it could make your job much harder than it needs to be.</p>
<ul>
<li><strong>Timer, Editor, Translator, Translation Checker</strong>
<ul>
<li>Some of the most forgiving roles in fansubbing for computer
hardware.</li>
<li><strong>OS</strong>: Windows 7, Mac OS X 10.7, Linux</li>
<li><strong>Screen</strong>: 720p</li>
<li><strong>CPU</strong>: dual-core &gt;2Ghz
<ul>
<li>Computer should be able to playback HD anime with
subtitles.</li>
</ul>
</li>
<li><strong>Memory</strong>: 4GB
<ul>
<li>Aegisub loads the entire video into memory. With larger HD
videos being standard today, this could be up to several GB.</li>
</ul>
</li>
<li><strong>Storage</strong>: 50GB available</li>
<li><strong>Mouse</strong>: recommended</li>
<li><strong>Internet</strong>: 25 Mbps download</li>
</ul>
</li>
<li><strong>Typesetter, Quality Checker</strong>
<ul>
<li>The middle of the road in terms of required computer hardware.</li>
<li><strong>OS</strong>: Windows 7, Mac OS X 10.7, Linux
<ul>
<li>64-bit recommended</li>
</ul>
</li>
<li><strong>Screen</strong>: 1080p</li>
<li><strong>CPU</strong>: dual-core &gt;2.5GHz (quad-core &gt;3GHz recommended)
<ul>
<li>Computer should be able to playback modern fansubbed anime
releases with high settings.</li>
</ul>
</li>
<li><strong>Memory</strong>: 8GB
<ul>
<li>Aegisub loads the entire video into memory. With larger HD
videos being standard today, this could be up to several GB.</li>
<li>Windows loads installed fonts into memory on boot. For
typesetters, the font library could grow to be several GB.</li>
</ul>
</li>
<li><strong>Storage</strong>: 100GB available</li>
<li><strong>Mouse</strong>: <em>required</em></li>
<li><strong>Internet</strong>: 25 Mbps download, 5 Mbps upload</li>
</ul>
</li>
<li><strong>Encoder</strong>
<ul>
<li>The most demanding role in terms of computer hardware.</li>
<li>The speed and capabilities of the computer directly correlate to
encode times and the stability of encoding tools.</li>
<li><strong>OS</strong>: Windows 7, Mac OS X 10.7, Linux
<ul>
<li>64-bit <em>required</em></li>
</ul>
</li>
<li><strong>Screen</strong>: 1080p
<ul>
<li>IPS panels highly recommended for color correctness.</li>
<li>VA panels highly discouraged.</li>
</ul>
</li>
<li><strong>CPU</strong>: quad-core &gt;4GHz
<ul>
<li>More cores and/or higher speed are better (e.g. AMD Ryzen,
Threadripper or Intel Core i7+).</li>
<li><strong>CPU Requirements</strong>:
<ul>
<li>Hyperthreading</li>
<li>AVX2</li>
<li>SSE4</li>
</ul>
</li>
</ul>
</li>
<li><strong>Memory</strong>: 8GB
<ul>
<li>Memory can be a bottleneck when encoding. More, faster
memory is always better for encoding rigs.</li>
</ul>
</li>
<li><strong>Storage</strong>: 500GB available
<ul>
<li>Encoders sometimes deal with files up to 40GB each and
regularly with ones between 1GB and 8GB and may be required
to retain these files for a long time.</li>
</ul>
</li>
<li><strong>Internet</strong>: 25 Mbps download, 25 Mbps upload</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#software" id="software">Software</a></h3>
<p>Every role will have different required software,
but it is recommended for every role to have installed <a href="http://www.aegisub.org">Aegisub</a>.
It is <strong>highly</strong> recommended to use <a href="https://thevacuumof.space/builds/">CoffeeFlux’s builds</a>.
They come pre-equipped with Dependency Control
and several <em>critical</em> fixes to Aegisub that have not been merged
into the official application.</p>
<p>More specifics will be presented in the chapters devoted to each role.</p>
<p>TODO - pages for each role</p>
<h3><a class="header" href="#programming" id="programming">Programming</a></h3>
<p>Prior knowledge of some programming languages
can be extremely useful for fansubbing, though it is not required.
Specifically, Lua and <a href="http://moonscript.org/">Moonscript</a> are useful for Typesetters.
Encoders will find that Python is used to interact with VapourSynth,
so learning it ahead of time will be to their advantage.</p>
<h1><a class="header" href="#preparation-and-necessary-software" id="preparation-and-necessary-software">Preparation and Necessary Software</a></h1>
<p>While the term “encoding” originally just referred
to the opposite of decoding—that is,
compressing raw video with a video codec—the term
has a broader meaning in the context of fansubbing.
Here, “encoding” includes the entire process
from receiving the source video until the final release.
Usual steps are processing or filtering
of the original video to remove defects,
compressing the video in a way that does not generate new artifacts,
transcoding audio to the desired format,
and muxing video,
audio,
subtitles,
fonts,
and other attachments into a container,
such as mkv.</p>
<p>Each of these steps requires different tools which will be listed and
explained in the following paragraphs.</p>
<p>It is assumed that you already have a source video at this point,
so software like torrent clients,
Perfect Dark,
Share,
or even FileZilla will not be covered.
If you don’t have a reliable way to get raws
and if your group doesn’t provide them,
try finding a source first.
Private bittorrent trackers like <a href="https://u2.dmhy.org" title="u2">u2</a>
or <a href="https://skyeysnow.com" title="SkyeySnow">SkyeySnow</a> are good starting points.</p>
<h2><a class="header" href="#processing-and-filtering" id="processing-and-filtering">Processing and Filtering</a></h2>
<h3><a class="header" href="#the-frameserver" id="the-frameserver">The Frameserver</a></h3>
<p>In order to process your source video
(which will be called “raw” throughout this chapter),
you need to import it into a so-called “frameserver”,
a software that is designed to process a video frame-by-frame,
usually based on a script that defines various filters
which will be applied to the video.</p>
<p>Currently, only two widely-known frameservers exist:
AviSynth and VapourSynth.</p>
<p>While many (especially older) encoders still use AviSynth,
there is no reason to use it
if you’re just starting to learn encoding.<sup class="footnote-reference"><a href="#1">1</a></sup>
Most AviSynth users only use it because
they have years of experience and
don’t want to switch.</p>
<p>Since this guide is aimed towards new encoders,
and the author has no qualms about imposing his own
opinions onto the host of people willing to listen,
the guide will focus on VapourSynth.
AviSynth equivalents are provided for certain
functions where applicable,
but the sample code will always be written for VapourSynth.</p>
<p>That being said, the installation of VapourSynth is quite easy.
It is strongly recommended to install the 64-bit version
of all tools listed here.
VapourSynth requires <a href="https://www.python.org/downloads/">Python 3.8.x or newer</a>.
VapourSynth Windows binaries can be found
<a href="https://github.com/vapoursynth/vapoursynth/releases">here</a>.
Linux users will have to build their own version,
but if you’re on Linux,
you probably know how to do that.
During the installation,
you might be prompted to install the Visual C++ Redistributables.
Just select “Yes” and the installer will do it for you.</p>
<p>And that’s it.
You can test your VapourSynth installation by opening the
Python shell and typing:</p>
<pre><code class="language-py">&gt;&gt;&gt; import vapoursynth
</code></pre>
<p>If the installation was not successful,
you should receive an error that reads:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&quot;, line 1, in &lt;module&gt;
ImportError: No module named 'vapoursynth'
</code></pre>
<p>In that case,
make sure your current Python shell is the correct version
(Python version as well as architecture),
try restarting your PC,
reinstall VapourSynth,
or <a href="https://discordapp.com/invite/ZB7ZXbN">ask for help</a>.</p>
<h3><a class="header" href="#plugins" id="plugins">Plugins</a></h3>
<p>In addition to VapourSynth’s core plugins,
community-created scripts and plugins
can be installed to extend the functionality of the frameserver.
These are usually more specific than the universally usable core plugins
or they are collections of wrappers and functions.
A (non-exhaustive) list of plugins and scripts is available in the
<a href="http://www.vapoursynth.com/doc/pluginlist.html" title="Plugins, Applications &amp; Scripts">official documentation</a>.
Additionally, an extensive database of VapourSynth plugins and scripts
is available at <a href="http://vsdb.top/">VSDB</a>.
VSDB also offers a GUI for <a href="https://github.com/vapoursynth/vsrepo">vsrepo</a>,
VapourSynth’s official package manager,
and a plugin pack (labeled “Portable FATPACK”)
that bundles most popular plugins,
scripts, and VS-related applications into one archive
for ease of download and installation.</p>
<p>An alternative to the latter is <a href="https://iamscum.wordpress.com/encoding-stuff/encode-pack/">eXmendiC’s encode pack</a>,
which contains a lot of encoding-related applications and scripts,
on top of a broad collection of VapourSynth plugins and scripts.
However, the package is not maintained anymore,
and may in part include outdated software.</p>
<h3><a class="header" href="#the-editor" id="the-editor">The Editor</a></h3>
<p>Now that you have installed the frameserver,
you can start filtering the video.
But without an editor,
you have no means of previewing the results other than test encodes
or raw output into a file.
That’s why editors exist.
They provide useful features such as autocompletion,
tooltips,
preview images,
and comparisons.</p>
<p>There are four editors that can be used to preview your
VapourSynth-Script.</p>
<ol>
<li>
<p><a href="https://bitbucket.org/mystery_keeper/vapoursynth-editor">VSEdit</a>.
It is a small editor software that can run on your PC. It can be
downloaded
<a href="https://bitbucket.org/mystery_keeper/vapoursynth-editor/downloads/">here</a>.
It provides an easy and simple GUI to write your VapourSynth
scripts.</p>
<p><img src="encoding/images/cnvimage100.png" alt="" />
<em>The main window of VSEdit.</em></p>
<p>While it seems to be unstable on some systems, its high performance
preview window offsets its problems.</p>
</li>
<li>
<p><a href="https://yuuno.encode.moe/">Yuuno</a>. While it is not an editor, Yuuno
is an extension to a Python-shell-framework that runs inside your
browser.
This increases latency, but it gives you a wider range of preview
related features while being more stable than VSEdit. It should be
noted that Yuuno natively supports remote access, as it is only an
extension for Jupyter Notebook.</p>
<p><img src="encoding/images/cnvimage101.png" alt="" />
<em>A Jupyter Notebook.</em></p>
</li>
<li>
<p><a href="https://github.com/mysteryx93/VapourSynthViewer.NET/releases/">VapourSynth Multi-Viewer</a>.
Multi-Viewer is a very simple and elementary previewing tool.
While the text editing is vastly inferior to VSEdit’s,
and the preview lacks a lot of VSEdit’s advanced features,
its tab-based previewing functionality easily outclasses
VSEdit’s single preview window,
because it makes comparisons between different script versions
a lot more convenient and efficient.</p>
<p>In short: very useful for thorough comparisons and filter fine-tuning,
not so much for everything else.</p>
<p><img src="encoding/images/VS-Multi-Viewer_Preview.png" alt="" />
<em>VS Multi-Viewer’s editing window.</em></p>
<p><img src="encoding/images/VS-Multi-Viewer_Editor.png" alt="" />
<em>VS Multi-Viewer’s preview window.</em></p>
</li>
<li>
<p><a href="https://github.com/AvsPmod/AvsPmod/releases">AvsPmod</a>. This is the
editor for AviSynth. It is old and slow but stable. When you are
using AviSynth, you are limited to this editor. AvsPmod <em>can</em> handle
AviSynth and VapourSynth scripts, however, VapourSynth support was
an afterthought and is therefore experimental, unstable, and
“hacky”.</p>
<p>Do not use AvsPmod for VapourSynth scripts unless you have a very
good reason!</p>
</li>
</ol>
<p>Please rest assured that the author does not impose any editor on you.
Instead we will give callouts for some editors.
These will be completely optional.</p>
<h2><a class="header" href="#video-codecs" id="video-codecs">Video Codecs</a></h2>
<p>Once you are happy with the result of your filter chain,
you want to save the output to a file.
While it is possible to store the script’s output as raw,
uncompressed pixel data,
that would result in hundreds of gigabytes of data for a single episode.
Because of this,
we use video codecs to compress the video.</p>
<p>Lossless compression will still result in very big files,
so we have to use lossy compression,
which means losing some information in the process.
As long as you’re not targeting unreasonable bitrates
(say, 50 MB per 24 minute episode),
this loss of information should be barely noticeable.
This process can be quite difficult,
so there will be an <a href="encoding/video-encoding.html">entire page</a> dedicated to it.</p>
<p>None of the encoders mentioned here need to be installed.
Just save the executable(s) somewhere for later.</p>
<p>For now, all you need to know is which codecs exist
and which encoders you want to use.</p>
<p>The codec used most commonly is h.264,
and the most popular h.264 encoder is x264.
The most recent builds can be found on <a href="https://artifacts.videolan.org/x264/">VideoLAN’s site</a>.
Pick the most recent build for your operating system.
At the time of writing this,
win64’s recent build is <code>x264-r2935-545de2f.exe</code>
from 25-Sep-2018.
(Notice: the 10-bit binaries are no longer separate from the 8-bit
as of <a href="https://code.videolan.org/videolan/x264/commit/71ed44c7">24 Dec 2017</a>,
meaning the releases with -10b can be ignored)
You can also build it locally from <a href="https://code.videolan.org/videolan/x264">the public repository</a>.</p>
<p>It used to be that different versions,
namely kmod and tmod,
were required for certain encoding features
such as <code>aq-mode 3</code>.
However, most relevant features have been
added to the upstream x264 builds.
Because of this, kmod is now unmaintained.
tmod is still being updated with changes from new x264 versions,
and it provides some potentially useful parameters such as <code>--fade-compensate</code>
or <code>--fgo</code> (film grain optimization), as well as additional AQ algorithms
(<code>aq2-mode</code>, <code>aq3-mode</code>, and parameters for these),
which are generally regarded as useless for almost all sources.
The current tmod release can be downloaded from <a href="https://github.com/jpsdr/x264/releases">the github page</a>.</p>
<p>A newer, more efficient alternative is HEVC,
with x265 being the most popular encoder.
It is still in active development
and aims for 20-50% lower bitrates with the same quality as x264.
It does have its flaws,
is a lot slower,
and not as widely supported by media players as x264,
but it can be a viable alternative,
especially if small files are important
and encoding time is of secondary importance.
Note that many groups will require you to use x264,
so ask your group leader before picking this encoder.</p>
<p>Other codecs, such as VP9,
are generally not used for fansubbing,
so they are not listed here.
The same is true for experimental codecs like <a href="https://xiph.org/daala/">Daala</a> and <a href="https://aomediacodec.github.io/av1-spec/">AV-1</a>.
Encoders made for distributed server encoding,
such as Intel’s <a href="https://github.com/OpenVisualCloud/SVT-AV1">SVT-AV1</a> will also not be included.</p>
<h2><a class="header" href="#audio" id="audio">Audio</a></h2>
<h3><a class="header" href="#audio-formats-and-how-to-handle-them" id="audio-formats-and-how-to-handle-them">Audio formats and how to handle them</a></h3>
<p>Depending on the source you’ll be working with,
you may encounter many different audio formats.</p>
<p>On Blu-rays,
you will most likely find audio encoded losslessly,
in the form of either DTS-HD Master Audio, Dolby TrueHD, or PCM.
DTS-HD MA and Dolby THD are proprietary codecs that use lossless compression,
while PCM is simply raw, uncompressed PCM data.
The usual way to handle these
is to re-encode them to other formats—either lossless or lossy,
depending on your taste.
But first, you need to decode them.
The recommended tool for that is FFmpeg.
You can find Windows builds and Linux packages
on <a href="https://www.ffmpeg.org/download.html">FFmpeg’s official site</a>.
It doesn’t need to be installed—you can just extract it somewhere.
But, since it is useful for many different tasks,
adding it to the system PATH is recommended.</p>
<p>When working with WEB and TV sources,
you will most likely have only lossy audio available.
The most common codecs here are AC-3, E-AC-3 and AAC.
Lossily compressed files should generally
not be re-encoded—the proper way to handle them
is to remux (i.e. copy) them to the final file.</p>
<h3><a class="header" href="#which-codecs-to-use" id="which-codecs-to-use">Which codecs to use?</a></h3>
<p>Once you have your lossless files decoded, you need to encode them.
Depending on your taste, you can choose a lossy or lossless codec.
The two most widely accepted codecs in fansubbing community are FLAC (lossless)
and AAC (lossy), but recently opus (also lossy) is gaining some popularity, too.</p>
<p>The recommended encoder for FLAC is the official one.
Download Windows builds from <a href="https://xiph.org/flac/download.html">xiph’s website</a>.
Most Linux distributions should have FLAC in their package repositories.</p>
<p>The recommended and most widely used AAC encoder is qaac,
available on <a href="https://sites.google.com/site/qaacpage/cabinet">its official site</a>.
Nero and Fraunhofer FDK aren’t really that much worse,
so you can use them if you really want.
Other AAC encoders are discouraged,
since they provide inferior results.</p>
<p>There is also opus, which is gaining some popularity recently.
It is currently the most efficient lossy codec,
and it’s completely FOSS if you’re into that.
The recommended opus encoder is the official one,
contained in the <a href="https://opus-codec.org/downloads/">opus-tools package</a>.</p>
<p>Just as with video,
these encoders don’t need to be installed.
Qaac will require some configuration, tho.</p>
<p>Other codecs are generally not recommended.
Formats like Monkey’s Audio and TAK provide very little gain over FLAC,
while not being as widely supported,
and—in the case of TAK—closed source.
DTS-HD MA and Dolby THD are much less efficient than FLAC,
and are also closed source.
MP3 is simply obsolete,
and Vorbis has been superseded by opus.
DTS and AC-3 provide even worse compression than MP3,
and don’t have any reasonable, free encoders.
In short—don’t bother,
unless you really have to, for some reason.</p>
<h3><a class="header" href="#lossless-or-lossy" id="lossless-or-lossy">Lossless or lossy?</a></h3>
<p>This is entirely dependent on you.
Some people like the idea of having a (theoretically)
perfect copy of the master audio file,
don’t mind the increase in size,
and state that lossless is the only way to go when archiving.
Others prefer smaller file sizes,
knowing that the difference—assuming high enough bitrate—won’t
be audible anyway.
And they both have a point.</p>
<p>So, do some ABX testing and decide for yourself.</p>
<h2><a class="header" href="#mkvtoolnix" id="mkvtoolnix">MKVToolNix</a></h2>
<p>You probably have at least three files now—that being the video,
audio,
and subtitles—and you need to combine all of them into a single file.
This process is called muxing.</p>
<p>MKVToolNix is used to mux all parts of the final output
into an mkv container.
Most people use MKVToolNix GUI,
which provides a graphical user interface to mux video,
audio,
chapters,
fonts,
and other attachments
into an mkv file.
Installation instructions for virtually any platform
can be found <a href="https://mkvtoolnix.download/downloads.html" title="MKVToolNix">on their website</a>.</p>
<p>It is possible to use other containers,
but Matroska has become the standard for video releases
due to its versatility and compatibility.</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>It should be noted that the author strongly disagrees with this sentiment.
The two have a lot in common,
and any capable AviSynth encoder could reach a similar level in Vapoursynth
within a few months, maybe even weeks.
At least I’m honest, okay?</p>
</div>
<h1><a class="header" href="#basics-and-general-workflow" id="basics-and-general-workflow">Basics and General Workflow</a></h1>
<h2><a class="header" href="#preparation" id="preparation">Preparation</a></h2>
<p>downloading a source, looking at the video, some decisions
(resolution(s) for the release, audio codec, group-specific
requirements)</p>
<h2><a class="header" href="#writing-the-script" id="writing-the-script">Writing the Script</a></h2>
<p>imports, source filter (mention lsmash, ffms2), examples for resizing,
debanding, AA. with images if possible</p>
<h2><a class="header" href="#encoding-the-result" id="encoding-the-result">Encoding the Result</a></h2>
<pre><code class="language-sh">$ vspipe.exe script.vpy -y - | x264.exe --demuxer y4m --some example --parameters here --output video.264 -
</code></pre>
<p>Editors for VapourSynth usually have inbuilt support for encoding
scripts you wrote. Use <code>%encode --y4m &lt;clip_variable&gt;</code> in Yuuno or the GUI
provided by VSEdit.</p>
<h2><a class="header" href="#transcoding-audio" id="transcoding-audio">Transcoding Audio</a></h2>
<p>examples for qaac, flac</p>
<h2><a class="header" href="#muxing" id="muxing">Muxing</a></h2>
<p>mkvtoolnix</p>
<h1><a class="header" href="#recognizing-video-artifacts" id="recognizing-video-artifacts">Recognizing Video Artifacts</a></h1>
<p>The term “artifact” is used to broadly describe defects or foreign,
unwanted elements in a video.
There can be any number of causes ranging from lossy compression,
improper conversions,
to post-processing adjustments like sharpening and resampling.
This guide will go over how to identify different types of artifacts,
how they occur,
and some steps to remove them wherever possible.</p>
<p>Before we dive into this laundry list of problems
with no clear solutions,
let’s start by recognizing some things
that often get mistaken for artifacts.</p>
<h2><a class="header" href="#not-artifacts" id="not-artifacts">Not Artifacts</a></h2>
<h3><a class="header" href="#grain" id="grain">Grain</a></h3>
<p>Aside from film grain,
grain is added to videos for a few different reasons.
It can be added by the studio to create an effect/change the atmosphere,
or it can be added by the disc author/encoder
to protect against more harmful artifacts from occurring after encoding
(mostly banding and blocking).
In excess,
it may be considered an artifact,
but to us anime encoders of the 21<sup>st</sup> century
it is not an artifact,
nor is it something we should be expected to remove.</p>
<p>However, it is often mistaken for “noise”,
and for anime and lossy compression,
these two things may sometimes be indistinguishable,
but the two differ at a fundamental level;
grain being added at the discretion of a human being<sup class="footnote-reference"><a href="#1">1</a></sup>,
and noise being added by lossy compression.
See the <a href="encoding/video-artifacts.html#noise">noise section</a> for more information on the subject.</p>
<h3><a class="header" href="#badly-drawn-line-art" id="badly-drawn-line-art">Badly drawn line art</a></h3>
<p>Bad lines happen,
but its hard to say whether it’s worth it to try to fix it.
Using awarpsharp or sangnom to fix it will surely lead to disaster.</p>
<p><img src="encoding/images/3cnvimage100.png" alt="" />
<em>Rakudai-Kishi-no-Cavalry-ep.01.png</em></p>
<h3><a class="header" href="#chromatic-aberration" id="chromatic-aberration">Chromatic Aberration</a></h3>
<p><img src="encoding/images/3cnvimage101.png" alt="" />
<em>Have I been staring at my monitor for too long?</em></p>
<h3><a class="header" href="#and-please-dont-do-this" id="and-please-dont-do-this">…and please don’t do this</a></h3>
<p><img src="encoding/images/3cnvimage102.png" alt="" />
<em>notevenonce.jpg</em></p>
<h2><a class="header" href="#artifacts" id="artifacts">Artifacts</a></h2>
<h3><a class="header" href="#blocking" id="blocking">Blocking</a></h3>
<p>DCT block based video compression has come a long way.
If you happen to be encoding an MPEG2 TV station
or a DVD from a previous decade,
you will likely come across something like this:</p>
<p><img src="encoding/images/blocky2.png" alt="" />
<em>Blocky Compression</em></p>
<p><img src="encoding/images/blocky1.jpg" alt="" />
<em>Blocky Exaggeration<sup class="footnote-reference"><a href="#2">2</a></sup></em></p>
<p>From Biamp’s Blog<sup class="footnote-reference"><a href="#2">2</a></sup>:
“Blocking is known by several names – including tiling,
jaggies,
mosaicing,
pixelating,
quilting,
and checkerboarding –
and it occurs whenever a complex (compressed) image
is streamed over a low bandwidth connection
(imagine a golf ball being passed through a garden hose).
At decompression,
the output of certain decoded blocks makes surrounding pixels
appear averaged together to look like larger blocks.” (Urban, 2017)</p>
<p>Thankfully most blocking in BDs and web streams nowadays isn’t
nearly as bad,
and can either be ignored or removed
by another stage in your filter chain<sup class="footnote-reference"><a href="#3">3</a></sup>.
Denoising,
debanding,
and adding grain will all help to reduce blocking.</p>
<h3><a class="header" href="#noise" id="noise">Noise</a></h3>
<p>As stated earlier,
noise and grain are often used interchangeably.
Visually, noise looks uglier and more out of place than grain.
It’s less defined and can look blotchy,
blocky,
or be made up of small dots,
whereas grain looks like a proper texture.</p>
<p>In some cases,
heavy random grain may be added and then encoded with a low bitrate,
resulting in large,
noisy,
blocky,
unstable grain in the video.
This is often impossible to remove without noticeable detail loss,
and in this case scenefiltering and heavy detail loss
are the only two options.</p>
<h3><a class="header" href="#banding" id="banding">Banding</a></h3>
<p><img src="encoding/images/banding.png" alt="" />
<em>Example image for banding</em></p>
<p>Due to with its many flat areas and smooth gradients,
banding is a frequent problem in anime,
which is caused by the limits of 8-bit color depth
and (especially in low bitrate sources) truncation.
The filter GradFun3 is the most common tool for removing it,
and is the right tool for the job in average cases.</p>
<p>Some other options are available if this isn’t enough:
Particularly large quantization errors,
worse banding in dark scenes,
and/or banding with grain are cases where
experimenting with a few masking/limiting techniques
or scene-filtering may be the best option.</p>
<h3><a class="header" href="#aliasing" id="aliasing">Aliasing</a></h3>
<p>Aliasing has a few main causes:
interlacing,
low bitrate encoding,
shitty sharpening,
and shitty upscaling
(the latter two are often accompanied by ringing).</p>
<p>In the case of resizing,
the descale plugin with the right settings
may be enough to alleviate the aliasing,
but bear in mind that the poorer your source video is,
the less effective it will be.</p>
<p>In other cases,
an edge directed interpolation filter,
normally used for deinterlacing,
is used to smooth the edges.
These include nnedi3,
eedi3,
EEDI2,
<del>and sangnom2</del>.
The process involves supersampling and
(re-)interpolating lines in an attempt to minimize detail loss and
maximize continuity.
Masking is very common,
and is always recommended.</p>
<p>There are one or two other methods,
the most common of which is a filter called daa.
It’s sometimes used,
but outside of bad interlacing-related aliasing,
it is rarely recommendable.</p>
<h3><a class="header" href="#ringing" id="ringing">Ringing</a></h3>
<p>Ringing is something of a blanket term for edge artifacts,
including mosquito noise,
edge enhancement artifacts,
overshoot,
or actual ring-like ringing caused by the Gibbs phenomenon.</p>
<p><img src="encoding/images/mosquito1.png" alt="" />
<em>Mosquito Noise<sup class="footnote-reference"><a href="#2">2</a></sup></em></p>
<p>In Blu-ray encodes,
the only ringing you’ll be likely to see is
upscaling methods such as Lanczos and sharp Bicubic variants,
or possibly from badly done sharpening.
This is because ringing is primarily a compression artifact,
and BDs are generally high bitrate,
and even bad BDs don’t tend to ring much.</p>
<p>Thus, you are much more likely to see ringing in low bitrate webrips
and MPEG2 TV captures.
Despite it being a vastly inferior codec,
ringing in MPEG2 sources is actually much easier to deal with than the
stubborn ringing in H.264 encodes.
In these cases,
a simple smoothing based edge-scrubber like HQDeringmod,
or a warpsharp-based scrubber <em>similar to</em> EdgeCleaner
it has a shitty mask) should all work just fine
without too many drawbacks.</p>
<p>In the case of heavily compressed H.264 sources,
consider doing a manual masking/limiting/filtering,
or scenefiltering with some of HQDeringmod’s safety-checks disabled
(change repair from default 24 to 23,
or disable entirely)</p>
<h3><a class="header" href="#haloing" id="haloing">Haloing</a></h3>
<p>Another edge artifact,
this time much cleaner and easier to spot.
Halos (especially in anime) are exactly as their title would imply;
an even,
thick,
brightness surrounding lines.
In some cases they might even seem like they’re supposed to be there.
In Blu-rays this is rarely a problem,
but if you do come across it,
a masked dehalo_alpha filter such as Fine_Dehalo
or a manual filtering of dehalo_alpha with dhhmask
(zzfunc.py coming soon™) are recommendable.</p>
<h3><a class="header" href="#cross-field-noise" id="cross-field-noise">Cross-Field Noise</a></h3>
<p><img src="encoding/images/3cnvimage103.png" alt="" />
<em>field-noise.jpg</em></p>
<p>TODO</p>
<h3><a class="header" href="#underflow--overflow" id="underflow--overflow">Underflow / Overflow</a></h3>
<p>While most of the anime produced
use the YUV 8-bit limited range<sup class="footnote-reference"><a href="#4">4</a></sup>,
we occasionally find some videos having the “limited range” flag set
while containing full range content.
This often results in oversaturated colors and weird brightness.
Thus, it is strongly recommended
to check the brightness levels
of the 8-bit source with <code>hist.Levels()</code>.</p>
<p><img src="encoding/images/underflow.jpg" alt="" />
<em>Example of underflow (<a href="https://slowpics.org/comparison/f125e799-fdff-4c4c-8c9d-707af021bd88">click for comparison</a>)</em></p>
<p><img src="encoding/images/overflow.jpg" alt="" />
<em>Example of overflow (<a href="https://slowpics.org/comparison/6e24ffe9-e068-4f33-b2e7-639031d512f2">click for comparison</a>)</em></p>
<p>To fix this problem,
simply use <a href="http://www.vapoursynth.com/doc/functions/resize.html"><code>resize</code></a> like so:</p>
<pre><code class="language-py"># Only applies to integer pixel formats, since floating point clips are always full range.
clip = clip.resize.Spline36(range_in_s=&quot;full&quot;, range_s=&quot;limited&quot;)
</code></pre>
<p>or set the “full range” flag on the video,
so the values can be interpreted accordingly.
Limited range video is more widely supported
and players may ignore the “full range” flag,
which results in interpreting full range content 
in a limited context.</p>
<p>In rare cases,
the issue may be more complicated.
For example,
a video may use faulty levels like 0-235 or 16-255
which are neither full nor limited range.
In such cases or similar,
<a href="http://www.vapoursynth.com/doc/functions/levels.html"><code>std.Levels</code></a> can be utilized to correct the range:</p>
<pre><code class="language-py"># This only applies to 8 bit clips!
# In this example, the input clip uses 0-235 for luma and 0-240 for chroma.
clip = clip.std.Levels(min_in=0, max_in=235, min_out=16, max_out=235, planes=0)      # y plane
clip = clip.std.Levels(min_in=0, max_in=240, min_out=16, max_out=240, planes=[1,2])  # u&amp;v planes
</code></pre>
<p>Because limited precision with only 8 bit per channel
may lead to rounding errors quickly,
we prefer adjusting the levels
(and our filtering in general)
with higher precision, 
such as 16 bit or float (32 bit).
In the example above,
you would use the following<sup class="footnote-reference"><a href="#5">5</a></sup>:</p>
<pre><code class="language-py"># 16 bit
clip = clip.std.Levels(min_in=0, max_in=235 &lt;&lt; 8, min_out=16 &lt;&lt; 8, max_out=235 &lt;&lt; 8, planes=0)      # y plane
clip = clip.std.Levels(min_in=0, max_in=240 &lt;&lt; 8, min_out=16 &lt;&lt; 8, max_out=240 &lt;&lt; 8, planes=[1,2])  # u&amp;v planes
</code></pre>
<p>An example for a case,
where shifting the levels with 8 bit precision
leads to rounding errors 
that may result in banding
and other weird artifacts,
can be seen below.</p>
<p><img src="encoding/images/overflow_notice.jpg" alt="" />
<em>When you see a histogram like this, increase precision.</em></p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>At least, in digital anime.
Actual grain is different but you most likely aren’t encoding shows from the 90s
so who cares.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Urban, J. (2017, February 16).
Understanding Video Compression Artifacts.
Retrieved from http://blog.biamp.com/understanding-video-compression-artifacts/</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>Blocking may also occur for other reasons
other than compression data loss.
<a href="https://github.com/nagadomi/waifu2x/issues/238">Image re-construction with padding</a>
can cause very similar looking effects,
although this is irrelevant for fansubbing source videos.</p>
</div>
<div class="footnote-definition" id="4"><sup class="footnote-definition-label">4</sup>
<p>The 8-bit limited range
(used in rec.601, rec.709, and BT.2020/2100)
only defines values within \([16,~235]\) for the Y
and \([16,~240]\) for the U and V planes.
This means that Y=16 is considered full black and Y=235 full white,
while any values outside of that range are clamped virtually (during rendering).
U and V behave analogously.</p>
</div>
<div class="footnote-definition" id="5"><sup class="footnote-definition-label">5</sup>
<p>The limited ranges in different precisions are shifted by
(multiplied by 2 to the power of) the added bits.
For 12-bit, for example, you multiply by \(2^{12-8}\),
resulting in \([256,~3760]\) and \([256,~3840]\) respectively.
The maximum value in full range is obviously the highest unsigned integer value,
so \(2^{12}-1\).</p>
</div>
<h1><a class="header" href="#scenefiltering" id="scenefiltering">Scenefiltering</a></h1>
<p>Scenefiltering can be hazardous to both your mind and body if used
extensively. Avoid scenefiltering if possible.</p>
<p>If you’re an aspiring young encoder or someone who has been around
fansubbing for a while, you’ve probably heard the term “scenefiltering”.
But what is scenefiltering? As the name suggests, it is simply filtering
different scenes or frames of a video clip distinctly.</p>
<h2><a class="header" href="#creating-the-base-filters" id="creating-the-base-filters">Creating the base filters</a></h2>
<p>Normally, if you have a source that has great video quality with minimal
video artifacts, you can use a simple chain of filters on the entire
video without any concern. However, if you have a more complex source
with a myriad of video artefacts, you probably don’t want to use the
same filters everywhere. For instance, one scene could have heavy
banding while another scene might have strong aliasing. If you were to
fix both of these issues by using strong filtering over the entire
video, it would likely result in detail loss in other scenes, which you
do not want. This is where scenefiltering comes in.</p>
<p>As always, you start by importing the VapourSynth module and loading
your video source:</p>
<pre><code class="language-py">import vapoursynth as vs  # this can look different based on your editor
core = vs.core

src = core.lsmas.LWLibavSource(&quot;source.m2ts&quot;)
</code></pre>
<p>Next, you need to choose what filtering will be done to the entire clip.
Some filtering—like resizing in this example—may
need to be put before any other filtering.
At this stage,
you can also come up with the default filters
that need to be in a certain order,
but will still be applied to the entire clip.
If you can’t come up with anything suitable, don’t fret;
you’ll have plenty more chances to filter later.</p>
<pre><code class="language-py">filtered = core.resize.Bilinear(src, width=1280, height=720)

# will occur at the deband stage, but for entire clip
default_deband = deband(filtered)
</code></pre>
<p>Now that you have your common filtering down, you need to create some
base filter chains. Go through some random scenes in your source and
write down parts of the filtering that best suits those scenes. You
should separate these as variables with proper names and sorting (group
filters by their type) to keep everything neat and clean. If you do this
part well, you will save yourself a lot of time later on, so take your
time. At this point, your script should look something like this:</p>
<pre><code class="language-py">import vapoursynth as vs
core = vs.core

src = core.lsmas.LWLibavSource(&quot;source.m2ts&quot;)
resized = core.resize.Bilinear(src, width=1280, height=720)

light_denoise = some_denoise_filter(resized)
heavy_denoise = some_other_denoise_filter(resized)

denoised = ...

aa = antialiasing(denoised)

aa = ...

default_deband = deband(aa)
light_deband   = deband1(aa)
medium_deband  = deband2(aa)

debanded = ...
</code></pre>
<h2><a class="header" href="#adding-the-frame-ranges" id="adding-the-frame-ranges">Adding the frame ranges</a></h2>
<p>Once you’ve done all of that, you’re done with filtering your source—at
least for the most part. Now all you need to do is add
<code>ReplaceFramesSimple</code> calls. For this, you need either the
plugin <a href="https://github.com/Irrational-Encoding-Wizardry/Vapoursynth-RemapFrames/releases">RemapFrames</a> or
the native Python version in
<a href="https://github.com/Irrational-Encoding-Wizardry/fvsfunc/blob/master/fvsfunc.py">fvsfunc</a><sup class="footnote-reference"><a href="#1">1</a></sup>.
<code>Rfs</code> is a shorthand for <code>ReplaceFramesSimple</code>
and fvsfunc has the alias <code>rfs</code>.</p>
<pre><code class="language-py">import vapoursynth as vs
core = vs.core

src = core.lsmas.LWLibavSource(&quot;source.m2ts&quot;)
resized = core.resize.Bilinear(src, width=1280, height=720)

### Denoising
light_denoise   = some_denoise_filter(resized)
heavy_denoise   = some_other_denoise_filter(resized)
heavier_denoise = some_stronger_denoise_filter(resized)

denoised = core.remap.Rfs(resized, light_denoise, mappings=&quot;&quot;)
denoised = core.remap.Rfs(denoised, heavy_denoise, mappings=&quot;&quot;)
denoised = core.remap.Rfs(denoised, heavier_denoise, mappings=&quot;&quot;)

### Anti-aliasing
eedi2_aa  = eedi2_aa_filter(denoised)
nnedi3_aa = nnedi3_aa_filter(denoised)

aa = core.remap.Rfs(denoised, eedi2_aa, mappings=&quot;&quot;)
aa = core.remap.Rfs(aa, nnedi3_aa, mappings=&quot;&quot;)

### Debanding
default_deband = default_deband(aa)
light_deband   = deband1(aa)
medium_deband  = deband2(aa)

debanded = default_deband  # will apply filter to the entire clip
debanded = core.remap.Rfs(debanded, light_deband, mappings=&quot;&quot;)
debanded = core.remap.Rfs(debanded, med_deband, mappings=&quot;&quot;)
</code></pre>
<p>So you created all your base filters and added Rfs calls. Now what? You
still have to perform the most tedious part of this entire
process—adding frame ranges to those calls. The basic workflow is
quite simple:</p>
<ol>
<li>
<p>Go to the start of the scene. View the next 2-3 frames. Go to the
end of the scene. View the previous 2-3 frames. Based on this,
decide on your filtering for the particular scene. If still in
doubt, look at other frames in the scene. Sometimes, you will find
that different frames in the same scene require different filtering,
but this is quite uncommon.</p>
</li>
<li>
<p>Now that you know what filter to use, simply add the frame range to
the respective Rfs call. To add a frame range to Rfs, you need to
enter it as a string in the <code>mappings</code> parameter. The format for the
string is <code>[start_frame end_frame]</code>. If you only want to add a
single frame, the format is <code>frame_number</code>. An example should help
you understand
better:</p>
<pre><code class="language-py"># The following replaces frames 30 to 40 (inclusive) and frame 50
# of the base clip with the filtered clip.
filtered = core.remap.Rfs(base, filtered, mappings=&quot;[30 40] 50&quot;)
</code></pre>
</li>
<li>
<p>Repeat with the next scene.</p>
</li>
</ol>
<p>When scenefiltering, it is good practice to comment out Rfs calls you’re
currently not using because they just make your script slower and eat up
memory.</p>
<p>This step can take anywhere from a few minutes to hours, depending on
the encoder and the source. Most of the time, the same filters can be
reused every episode with some minor changes here and there.</p>
<p>Now you might ask, “Why did I have to create base filters for
everything?” The answer is that these base filters allow other filters
to be added on top of them. Let’s say a scene requires <code>light_denoise</code>
but also needs <code>medium_deband</code> on top of that. Just put the same frame
ranges in their Rfs calls and watch it happen. What if a scene requires
denoising stronger than <code>heavier_denoise</code> ? Simple. Add another denoising
filter instead of <code>heavier_denoise</code> like so:</p>
<pre><code class="language-py">super_heavy_denoise = ultra_mega_super_heavy_denoise(filtered)

filtered = core.remap.Rfs(filtered, super_heavy_denoise, mappings=&quot;[x y]&quot;)
</code></pre>
<p>Using different denoisers on that same frame range is also possible, but
always consider the impacts on performance. Calling a strong, slow
denoise filter might still be faster (and better-looking) than calling a
weak, faster filter multiple times.</p>
<h3><a class="header" href="#editor-shortcuts--tips" id="editor-shortcuts--tips">Editor shortcuts / tips</a></h3>
<p>If using VSEdit as your <a href="encoding/preparation.html#the-editor">editor</a>,
it can be helpful to use the
built-in bookmark functionality
to find the frame ranges of each scene.
There is a <a href="https://gist.github.com/OrangeChannel/b9666b3650a3448589069d25dd6a394c">small script</a> that can generate
these bookmarks from your clip inside of VSEdit.
If you already have a keyframe file
(WWXD qp-file or Xvid keyframes)
you can instead use the <code>convert</code> function.</p>
<pre><code class="language-py"># Editing a script called 'example01.vpy'
import ...
from vsbookmark import generate

generate(clip, 'example01')
#convert('keyframes.txt', 'example01')
clip.set_output()
</code></pre>
<p>When previewing your clip,
there will now be bookmarks generated on the timeline
allowing you to skip to the next scene using the GUI buttons.</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>The python script may be slower than the plug-in due to the way it calls std.Splice to combine multiple re-mappings. The plug-in on the other hand, directly serves the frames of the second clip, with no calls to Splice. The speed difference will likely only be noticeable with a large amount of re-mappings. So, for the average script, it should be unnoticeable.</p>
</div>
<h1><a class="header" href="#masking-limiting-and-related-functions" id="masking-limiting-and-related-functions">Masking, Limiting, and Related Functions</a></h1>
<p>There are filters
which change the video in various ways,
and then there are ways to change the filtering itself.
There are likely hundreds of different techniques at your disposal
for various situations,
using masks to protect details from smoothing filters,
blending two clips with different filtering applied,
and countless others—many of which haven’t been thought of yet.
This article will cover:</p>
<ul>
<li>Masking and Merging</li>
<li>Limiting</li>
<li>Reference clips</li>
<li>Expressions and Lookup Tables</li>
<li>Runtime functions</li>
<li>Pre-filtering</li>
</ul>
<h2><a class="header" href="#masking" id="masking">Masking</a></h2>
<p>Masking refers to a broad set of techniques used to merge multiple clips.
Usually one filtered clip is merged with a source clip
according to an overlay mask clip.
A mask clip specifies the weight for each individual pixel
according to which the two clips are merged;
see <a href="encoding/masking-limiting-etc.html#stdmaskedmerge">MaskedMerge</a> for details.</p>
<p>In practice,
masks are usually used to protect details,
texture, and/or edges
from destructive filtering effects like smoothing;
this is accomplished
by masking the areas to protect,
e.g. with an edgemask,
and merging the filtered clip with the unfiltered clip
according to the mask,
such that the masked areas are taken from the unfiltered clip,
and the unmasked areas are taken from the filtered clip.
In effect,
this applies the filtering only to the unmasked areas of the clip,
leaving the masked details/edges intact.</p>
<p>Mask clips are usually grayscale,
i.e. they consist of only one plane and thus contain no color information.
In VapourSynth, such clips use the color family <code>GRAY</code>
and one of these formats:
<code>GRAY8</code> (8 bits integer),
<code>GRAY16</code> (16 bits integer),
or <code>GRAYS</code> (single precision floating point).</p>
<h4><a class="header" href="#a-hrefhttpwwwvapoursynthcomdocfunctionsmaskedmergehtmlstdmaskedmergea" id="a-hrefhttpwwwvapoursynthcomdocfunctionsmaskedmergehtmlstdmaskedmergea"><a href="http://www.vapoursynth.com/doc/functions/maskedmerge.html">std.MaskedMerge</a></a></h4>
<p>This is the main function for masking
that performs the actual merging.
It takes three clips as input:
two source clips and one mask clip.
The output will be a convex combination of the input clips,
where the weights are given by the brightness of the mask clip.
The following formula
describes these internals for each pixel:</p>
<p>$$
\mathrm{output} = \mathrm{clip~a} \times (\mathit{max~value} - \mathrm{mask}) + (\mathrm{clip~b} \times \mathrm{mask})
$$</p>
<p>where \(\mathit{max~value}\) is 255 for 8-bit.</p>
<p>In simpler terms:
for brighter areas in the mask,
the output will come from <strong>clip b</strong>,
and for the dark areas,
it’ll come from <strong>clip a</strong>.
Grey areas result in an average of <strong>clip a</strong> and <strong>clip b</strong>.</p>
<p>If <code>premultiplied</code> is set to True,
the equation changes as follows:</p>
<p>$$
\mathrm{output} = \mathrm{clip~a} \times (\mathit{max~value} - \mathrm{mask}) + \mathrm{clip~b}
$$</p>
<hr />
<h3><a class="header" href="#manipulating-masks" id="manipulating-masks">Manipulating Masks</a></h3>
<p>Building precise masks
that cover exactly what you want
is often rather tricky.
VapourSynth provides basic tools for manipulating masks
that can be used to bring them into the desired shape:</p>
<h4><a class="header" href="#a-hrefhttpwwwvapoursynthcomdocfunctionsminimum_maximumhtmlstdminimumstdmaximuma" id="a-hrefhttpwwwvapoursynthcomdocfunctionsminimum_maximumhtmlstdminimumstdmaximuma"><a href="http://www.vapoursynth.com/doc/functions/minimum_maximum.html">std.Minimum/std.Maximum</a></a></h4>
<p>The Minimum/Maximum operations replace each pixel
with the smallest/biggest value in its 3x3 neighbourhood.
The 3x3 neighbourhood of a pixel
are the 8 pixels directly adjacent to the pixel in question
plus the pixel itself.</p>
<p><img src="encoding/images/3x3.png" alt="" />
<em>Illustration of the 3x3 neighborhood</em></p>
<p>The Minimum/Maximum filters
look at the 3x3 neighbourhood of each pixel in the input image
and replace the corresponding pixel in the output image
with the brightest (Maximum) or darkest (Minimum) pixel in that neighbourhood.</p>
<p>Maximum generally expands/grows a mask
because all black pixels adjacent to white edges will be turned white,
whereas Minimum generally shrinks the mask
because all white pixels bordering on black ones will be turned black.</p>
<p>See the next section for usage examples.</p>
<p>Side note:
In general image processing,
these operations are known as <a href="https://en.wikipedia.org/wiki/Erosion_(morphology)">Erosion</a> (Minimum)
and <a href="https://en.wikipedia.org/wiki/Dilation_(morphology)">Dilation</a> (Maximum).
Maximum/Minimum actually implement only a specific case
where the <a href="https://en.wikipedia.org/wiki/Structuring_element">structuring element</a> is a 3x3 square.
The built-in <code>morpho</code> plug-in implements the more general case
in the functions <code>morpho.Erode</code> and <code>morpho.Dilate</code>
which allow finer control over the structuring element.
However, these functions are significantly slower than
<code>std.Minimum</code> and <code>std.Maximum</code>.</p>
<h4><a class="header" href="#a-hrefhttpwwwvapoursynthcomdocfunctionsdeflate_inflatehtmlstdinflatestddeflatea" id="a-hrefhttpwwwvapoursynthcomdocfunctionsdeflate_inflatehtmlstdinflatestddeflatea"><a href="http://www.vapoursynth.com/doc/functions/deflate_inflate.html">std.Inflate/std.Deflate</a></a></h4>
<p>TODO</p>
<h4><a class="header" href="#a-hrefhttpwwwvapoursynthcomdocfunctionsbinarizehtmlstdbinarizea" id="a-hrefhttpwwwvapoursynthcomdocfunctionsbinarizehtmlstdbinarizea"><a href="http://www.vapoursynth.com/doc/functions/binarize.html">std.Binarize</a></a></h4>
<p>Split the luma/chroma values of any clip into one of two values,
according to a fixed threshold.
For instance,
binarize an edgemask to white when edge values are at or above 24,
and set values lower to 0:</p>
<pre><code class="language-py">mask.std.Binarize(24, v0=0, v1=255)
</code></pre>
<p>For methods of creating mask clips,
there are a few general categories…</p>
<h3><a class="header" href="#line-masks" id="line-masks">Line masks</a></h3>
<p>These are used for normal edge detection,
which is useful for processing edges or the area around them,
like anti-aliasing and deringing.
The traditional edge detection technique is
to apply one or more convolutions,
focused in different directions,
to create a clip containing what you might call a gradient vector map,
or more simply a clip which has brighter values in pixels
where the neighborhood dissimilarity is higher.
Some commonly used examples would be <strong>Prewitt</strong> (core),
<strong>Sobel</strong> (core),
and <strong>kirsch</strong> (kagefunc).</p>
<p>There are also some edge detection methods that use pre-filtering
when generating the mask.
The most common of these would be <strong>TCanny</strong>,
which applies a Gaussian blur before creating a 1-pixel-thick Sobel mask.
The most noteworthy pre-processed edge mask would be kagefunc’s
<strong>retinex_edgemask</strong> filter,
which at least with cartoons and anime,
is unmatched in its accuracy.
This is the mask to use if you want edge
masking with ALL of the edges and nothing BUT the edges.</p>
<p>Another edge mask worth mentioning is the mask in dehalohmod,
which is a black-lineart mask well-suited to dehalo masking.
Internally it uses a mask called a Camembert to generate a larger mask
and limits it to the area affected by a line-darkening script.
The main mask has no name and is simply dhhmask(mode=3).</p>
<p>For more information about edgemasks,
see <a href="https://kageru.moe/blog/article/edgemasks">kageru’s blog post</a>.</p>
<p>The <strong>range mask</strong>
(or in masktools,
the “min/max” mask)
also fits into this category.
It is a very simple masking method that
returns a clip made up of the maximum value of a range of neighboring pixels
minus the minimum value of the range,
as so:</p>
<pre><code class="language-py">clipmax = core.std.Maximum(clip)
clipmin = core.std.Minimum(clip)
minmax = core.std.Expr([clipmax, clipmin], 'x y -')
</code></pre>
<p>The most common use of this mask is within GradFun3.
In theory,
the neighborhood variance technique is the perfect fit for a debanding mask.
Banding is the result of 8 bit color limits,
so we mask any pixel with a neighbor higher or lower than one 8 bit color step,
thus masking everything except potential banding.
But alas,
grain induces false positives
and legitimate details within a single color step are smoothed out,
therefore debanding will forever be a balancing act between
detail loss and residual artifacts.</p>
<h4><a class="header" href="#example-build-a-simple-dehalo-mask" id="example-build-a-simple-dehalo-mask">Example: Build a simple dehalo mask</a></h4>
<p>Suppose you want to remove these halos:</p>
<p><img src="encoding/images/halos.png" alt="" />
<em>Screenshot of the source.</em></p>
<p><img src="encoding/images/src0.png" alt="" />
<em>Point-enlargement of the halo area.</em></p>
<p>(Note that the images shown in your browser are likely resized poorly;
you can view them at full size in <a href="https://slowpics.org/comparison/96cbeca4-b4be-4dfc-82b1-631bbc85cdb0">this comparison</a>.)</p>
<p>Fortunately, there is a well-established script that does just that:
<a href="https://github.com/HomeOfVapourSynthEvolution/havsfunc/blob/8b2cd62a20faf0b410c742c95e7c7848894628d4/havsfunc.py#L370">DeHalo_alpha</a>.</p>
<p>However, we must be cautious in applying that filter,
since, while removing halos reliably,
it’s extremely destructive to the lineart as well.
Therefore we must use a <strong>dehalo mask</strong>
to protect the lineart and limit the filtering to halos.</p>
<p>A dehalo mask aims to cover the halos
but exclude the lines themselves,
so that the lineart won’t be blurred or dimmed.
In order to do that,
we first need to generate an edgemask.
In this example,
we’ll use the built-in Sobel function.
After generating the edge mask, we extract the luma plane:</p>
<pre><code class="language-py">mask = core.std.Sobel(src, 0)
luma = core.std.ShufflePlanes(mask, 0, colorfamily=vs.GRAY)
</code></pre>
<p><img src="encoding/images/luma0.png" alt="" />
<em><code>luma</code></em></p>
<p>Next, we expand the mask twice, so that it covers the halos.
<code>vsutil.iterate</code> is a <a href="https://github.com/Irrational-Encoding-Wizardry/vsutil/blob/c0206e2b68357fcbcfbcb47fafec70cce8391786/vsutil.py#L64">function in vsutil</a>
which applies the specified filter a specified number of times
to a clip—in this case it runs <code>std.Maximum</code> 2 times.</p>
<pre><code class="language-py">mask_outer = vsutil.iterate(luma, core.std.Maximum, 2)
</code></pre>
<p><img src="encoding/images/mask_outer0.png" alt="" />
<em><code>mask_outer</code></em></p>
<p>Now we shrink the expanded clip back
to cover only the lineart.
Applying <code>std.Minimum</code> twice
would shrink it back to the edge mask’s original size,
but since the edge mask covers part of the halos too,
we need to erode it a little further.</p>
<p>The reason we use <code>mask_outer</code> as the basis and shrink it thrice,
instead of using <code>mask</code> and shrinking it once,
which would result in a similar outline,
is that this way,
small adjacent lines with gaps in them
(i.e. areas of fine texture or details),
such as the man’s eyes in this example,
are covered up completely,
preventing detail loss.</p>
<pre><code class="language-py">mask_inner = vsutil.iterate(mask_outer, core.std.Minimum, 3)
</code></pre>
<p><img src="encoding/images/mask_inner0.png" alt="" />
<em><code>mask_inner</code></em></p>
<p>Now we subtract the outer mask covering the halos
and the lineart from the inner mask covering only the lineart.
This yields a mask covering only the halos,
which is what we originally wanted:</p>
<pre><code class="language-py">halos = core.std.Expr([mask_outer, mask_inner], 'x y -')
</code></pre>
<p><img src="encoding/images/halos0.png" alt="" />
<em><code>halos</code></em></p>
<p>Next, we do the actual dehaloing:</p>
<pre><code class="language-py">dehalo = hf.DeHalo_alpha(src)
</code></pre>
<p><img src="encoding/images/dh0.png" alt="" />
<em><code>dehalo</code></em></p>
<p>Lastly, we use MaskedMerge to merge only the filtered halos
into the source clip,
leaving the lineart mostly untouched:</p>
<pre><code class="language-py">masked_dehalo = core.std.MaskedMerge(src, dehalo, halos)
</code></pre>
<p><img src="encoding/images/dehalod0.png" alt="" />
<em><code>masked_dehalo</code></em></p>
<hr />
<h3><a class="header" href="#diff-masks" id="diff-masks">Diff masks</a></h3>
<p>A diff(erence) mask is any mask clip
generated using the variance of two clips.
There are many different ways to use this type of mask:
limiting a difference to a threshold,
processing a filtered difference itself,
or smoothing →
processing the clean clip →
overlaying the original grain.
They can also be used in conjunction with
line masks,
for example:
kagefunc’s hardsubmask uses a special edge mask with a diff mask,
and uses core.misc.Hysteresis to grow the line mask into diff mask.</p>
<h4><a class="header" href="#example-create-a-descale-mask-for-white-non-fading-credits-with-extra-protection-for-lines-16-bit-input" id="example-create-a-descale-mask-for-white-non-fading-credits-with-extra-protection-for-lines-16-bit-input">Example: Create a descale mask for white non-fading credits with extra protection for lines (16 bit input)</a></h4>
<pre><code class="language-py">src16 = kgf.getY(last)
src32 = fvf.Depth(src16, 32)

standard_scale = core.resize.Spline36(last, 1280, 720, format=vs.YUV444P16, resample_filter_uv='spline16')

inverse_scale = core.descale.Debicubic(src32, 1280, 720)
inverse_scale = fvf.Depth(inverse_scale, 16)

#absolute error of descaling
error = core.resize.Bicubic(inverse_scale, 1920, 1080)
error = core.std.Expr([src, error], 'x y - abs')

#create a light error mask to protect smaller spots against halos aliasing and rings
error_light = core.std.Maximum(error, coordinates=[0,1,0,1,1,0,1,0])
error_light = core.std.Expr(error_light, '65535 x 1000 / /')
error_light = core.resize.Spline36(error_light, 1280, 720)

#create large error mask for credits, limiting the area to white spots
#masks are always full-range, so manually set fulls/fulld to True or range_in/range to 1 when changing bitdepth
credits = core.std.Expr([src16, error], 'x 55800 &gt; y 2500 &gt; and 255 0 ?', vs.GRAY8)
credits = core.resize.Bilinear(credits, 1280, 720)
credits = core.std.Maximum(credits).std.Inflate().std.Inflate()
credits = fvf.Depth(credits, 16, range_in=1, range=1)

descale_mask = core.std.Expr([error_light, credits], 'x y -')

output = kgf.getY(standard_scale).std.MaskedMerge(inverse_scale, descale_mask)
output = muvf.MergeChroma(output, standard_scale)
</code></pre>
<h2><a class="header" href="#single-and-multi-clip-adjustments-with-stdexpr-and-friends" id="single-and-multi-clip-adjustments-with-stdexpr-and-friends">Single and multi-clip adjustments with std.Expr and friends</a></h2>
<p>VapourSynth’s core contains many such filters,
which can manipulate one to three different clips according to a math function.
Most, if not all,
can be done (though possibly slower) using std.Expr,
which will be
covered at the end of this
sub-section.</p>
<h4><a class="header" href="#a-hrefhttpwwwvapoursynthcomdocfunctionsmakediffhtmlstdmakediffa-and-a-hrefhttpwwwvapoursynthcomdocfunctionsmergediffhtmlstdmergediffa" id="a-hrefhttpwwwvapoursynthcomdocfunctionsmakediffhtmlstdmakediffa-and-a-hrefhttpwwwvapoursynthcomdocfunctionsmergediffhtmlstdmergediffa"><a href="http://www.vapoursynth.com/doc/functions/makediff.html">std.MakeDiff</a> and <a href="http://www.vapoursynth.com/doc/functions/mergediff.html">std.MergeDiff</a></a></h4>
<p>Subtract or add the difference of two clips, respectively.
These filters are peculiar in that they work differently in
integer and float formats,
so for more complex filtering
float is recommended whenever possible.
In 8 bit integer format where neutral luminance (gray) is 128,
the function is \(\mathrm{clip~a} - \mathrm{clip~b} + 128\) for MakeDiff
and \(\mathrm{clip~a} + \mathrm{clip~b} - 128\) for MergeDiff,
so pixels with no change will be gray.</p>
<p>The same is true of 16 bit and 32768.
The float version is simply \(\mathrm{clip~a} - \mathrm{clip~b}\) so in 32 bit
the difference is defined normally,
negative for dark differences,
positive for bright differences,
and null differences are zero.</p>
<p>Since overflowing values are clipped to 0 and 255,
changes greater than 128 will be clipped as well.
This can be worked around by re-defining the input clip as so:</p>
<pre><code class="language-py">smooth = core.bilateral.Bilateral(src, sigmaS=6.4, sigmaR=0.009)
noise = core.std.MakeDiff(src, smooth) # subtract filtered clip from source leaving the filtered difference
smooth = core.std.MakeDiff(src, noise) # subtract diff clip to prevent clipping (doesn't apply to 32 bit)
</code></pre>
<h4><a class="header" href="#a-hrefhttpwwwvapoursynthcomdocfunctionsmergehtmlstdmergea" id="a-hrefhttpwwwvapoursynthcomdocfunctionsmergehtmlstdmergea"><a href="http://www.vapoursynth.com/doc/functions/merge.html">std.Merge</a></a></h4>
<p>This function is similar to <a href="encoding/masking-limiting-etc.html#stdmaskedmerge">MaskedMerge</a>,
the main difference being
that a constant weight is supplied
instead of a mask clip to read the weight from for each pixel.
The formula is thus just as simple:</p>
<p>$$
\mathrm{output} = \mathrm{clip~a} \times (\mathit{max~value} - \mathrm{weight}) + (\mathrm{clip~b} \times \mathrm{weight})
$$</p>
<p>It can be used to perform
a weighted average of two clips or planes.</p>
<h4><a class="header" href="#a-hrefhttpwwwvapoursynthcomdocfunctionsexprhtmlstdexpra" id="a-hrefhttpwwwvapoursynthcomdocfunctionsexprhtmlstdexpra"><a href="http://www.vapoursynth.com/doc/functions/expr.html">std.Expr</a></a></h4>
<p>TODO</p>
<h4><a class="header" href="#a-hrefhttpwwwvapoursynthcomdocfunctionsluthtmlstdluta-and-a-hrefhttpwwwvapoursynthcomdocfunctionslut2htmlstdlut2a" id="a-hrefhttpwwwvapoursynthcomdocfunctionsluthtmlstdluta-and-a-hrefhttpwwwvapoursynthcomdocfunctionslut2htmlstdlut2a"><a href="http://www.vapoursynth.com/doc/functions/lut.html">std.Lut</a> and <a href="http://www.vapoursynth.com/doc/functions/lut2.html">std.Lut2</a></a></h4>
<p>May be slightly faster than Expr in some cases,
otherwise they can’t really do anything that Expr can’t.
You can substitute a normal Python function for the RPN expression, though,
so you may still find it easier.
See link for usage information.</p>
<h2><a class="header" href="#limiting" id="limiting">Limiting</a></h2>
<p>TODO</p>
<h2><a class="header" href="#referencing" id="referencing">Referencing</a></h2>
<p>TODO</p>
<h2><a class="header" href="#runtime-filtering-with-frameeval" id="runtime-filtering-with-frameeval">Runtime filtering with FrameEval</a></h2>
<p>TODO</p>
<h4><a class="header" href="#example-strong-smoothing-on-scene-changes-ie-for-mpeg-2-transport-streams" id="example-strong-smoothing-on-scene-changes-ie-for-mpeg-2-transport-streams">Example: Strong smoothing on scene changes (i.e. for MPEG-2 transport streams)</a></h4>
<pre><code class="language-py">from functools import partial

src = core.d2v.Source()
src = ivtc(src)
src = haf.Deblock_QED(src)

ref = core.rgvs.RemoveGrain(src, 2)

# xvid analysis is better in lower resolutions
first = core.resize.Bilinear(ref, 640, 360).wwxd.WWXD()
# shift by one frame
last = core.std.DuplicateFrames(first, src.num_frames - 1).std.DeleteFrames(0)

# copy prop to last frame of previous scene
propclip = core.std.ModifyFrame(first, clips=[first, last], selector=shiftback)

def shiftback(n, f):
    both = f[0].copy()
    if f[1].props.SceneChange == 1:
        both.props.SceneChange = 1
    return both

def scsmooth(n, f, clip, ref):
    if f.props.SceneChange == 1:
        clip = core.dfttest.DFTTest(ref, tbsize=1)
    return clip

out = core.std.FrameEval(src, partial(scsmooth, clip=src, ref=ref), prop_src=propclip)
</code></pre>
<h2><a class="header" href="#pre-filters" id="pre-filters">Pre-filters</a></h2>
<p>TODO</p>
<h4><a class="header" href="#example-deband-a-grainy-clip-with-f3kdb-16-bit-input" id="example-deband-a-grainy-clip-with-f3kdb-16-bit-input">Example: Deband a grainy clip with f3kdb (16 bit input)</a></h4>
<pre><code class="language-py">src16 = last
src32 = fvf.Depth(last, 32)

# I really need to finish zzfunc.py :&amp;lt;
minmax = zz.rangemask(src16, rad=1, radc=0)

#8-16 bit MakeDiff and MergeDiff are limited to 50% of full-range, so float is used here
clean = core.std.Convolution(src32, [1,2,1,2,4,2,1,2,1]).std.Convolution([1]*9, planes=[0])
grain = core.std.Expr([src32, clean32], 'x y - 0.5 +')

clean = fvf.Depth(clean, 16)
deband =core.f3kdb.Deband(clean, 16, 40, 40, 40, 0, 0, keep_tv_range=True, output_depth=16)

#limit the debanding: f3kdb becomes very strong on the smoothed clip (or rather, becomes more efficient)
#adapt strength according to a neighborhood-similarity mask, steadily decreasing strength in more detailed areas
limited = zz.AdaptiveLimitFilter(deband, clean, mask=minmax, thr1=0.3, thr2=0.0, mthr1=400, mthr2=700, thrc=0.25)

output = fvf.Depth(limited, 32).std.MergeDiff(grain)
</code></pre>
<h1><a class="header" href="#descaling" id="descaling">Descaling</a></h1>
<p>The ability to descale is a wonderful tool
to have in any encoder’s arsenal.
You may have heard before that
most anime are not native 1080p,
but a lower resolution.
But how do we make use of that?
How do you find the native resolution
and reverse the upscale?</p>
<h2><a class="header" href="#when-and-where-to-descale" id="when-and-where-to-descale">When and where to descale</a></h2>
<p>There are many circumstances
where descaling might prove beneficial.
For example,
say you’ve got
a very blurry Blu-ray source.
Rather than sharpening it,
you might want to consider
checking if it’s possible to descale it
and maybe alleviate a lot of
the blur that way.
Or the opposite:
say you’ve got a source full of ringing.
It might have been upscaled
using a very sharp kernel,
so it’s worth a try
to see if it can be descaled.
It’s no surprise that descaling tends to
offer far better lineart
than usual rescaling does.</p>
<p>However, descaling is not always an option.
The worse your source is,
the less likely it is that
descaling will yield better results
than a simple resample.
If you’ve got a source
full of broken gradients
or noise patterns,
like your average simulcast stream,
descaling might only hurt the overall quality.
Sadly, sources with a lot of post-processing
might also prove tough to properly descale
without dabbling with specific masks.
However, as long as you’ve got
a source with nice, clean lineart,
descaling might be a viable option,
and possibly nullify the need
for a lot of other filtering.</p>
<h2><a class="header" href="#preparation-1" id="preparation-1">Preparation</a></h2>
<p>To prepare for figuring out the native resolution,
you’ll want to use <a href="https://github.com/Infiziert90/getnative">getnative</a>,
a Python script designed for
figuring out the resolution
a show was animated at.
For the actual descaling,
make sure to grab BluBb-mADe’s <a href="https://github.com/BluBb-mADe/vapoursynth-descale">descale</a>.</p>
<p>One important thing
to keep in mind when descaling
is that you will never find
“the perfect descaling settings”.
Even if you find the exact settings
that the studio used,
you won’t be able to get a frame-perfect replica
of the original frame.
This is because the best available sources to consumers,
usually Blu-rays,
aren’t lossless.
There are always going to be some differences
from the original masters
which makes it impossible
to perfectly descale something.
However, usually those differences
are so small that they’re negligible.
If you run into a case where
you can’t find any low relative error spikes,
descaling can be highly destructive.
It’s instead recommended to resize
as you would normally,
or to not mess with scaling at all.</p>
<h2><a class="header" href="#finding-out-the-native-resolution" id="finding-out-the-native-resolution">Finding out the native resolution</a></h2>
<p>To figure out what
the native resolution of an anime is,
first you need a good frame to test.
Ideally,
you’ll want a bright frame
with as little blur as possible of high quality
(Blu-ray or very good webstreams).
It also helps to not have
too many post-processed elements in the picture.
Whilst it is most definitely possible
to get pretty good results with “bad” frames,
it’s generally better to use
good frames whenever possible.</p>
<p>Here are some examples of “bad” frames.</p>
<p><img src="encoding/images/descale_manaria01.png" alt="" />
<em>Manaria Friends — 01 (frame 1)</em></p>
<p>This picture is dark.
It also has some effects over it.</p>
<p><img src="encoding/images/descale_manaria02.png" alt="" />
<em>Manaria Friends — 01 (frame 2)</em></p>
<p>This picture is also very dark
and has even more effects over it.</p>
<p><img src="encoding/images/descale_manaria03.png" alt="" />
<em>Manaria Friends — 01 (frame 3)</em></p>
<p>Heavy dynamic grain will almost always give bad results.</p>
<p><img src="encoding/images/descale_manaria04.png" alt="" />
<em>Manaria Friends — 01 (frame 4)</em></p>
<p>This is a nice frame to use as reference.
The background is a bit blurry,
but it isn’t full of effects
and is fairly bright.
The lineart is very clear.</p>
<p>We will now make use of the getnative.py script
to figure out what resolution
this anime was produced at.
Run the following in your terminal:</p>
<pre><code class="language-bash">$ python getnative.py &quot;descale_manaria04.png&quot;
</code></pre>
<p>It should show the following:</p>
<pre><code>Using imwri as source filter
501/501
Kernel: bicubic AR: 1.78 B: 0.33 C: 0.33
Native resolution(s) (best guess): 878p
done in 18.39s
</code></pre>
<p>If you check the directory
where you executed the script,
you will find a new folder
called “getnative”.
You can find the following graph
in there as well:</p>
<p><img src="encoding/images/descale_graph.png" alt="" />
<em>Manaria Friends — 01 (frame 4 getnative graph)</em></p>
<p>The X-axis shows the resolutions that were checked,
and the Y-axis shows the relative error.
The relative error refers to
the difference between the original frame
and the rescaled frame.
What you’re looking for are
the spikes that show a low relative error.
In this case it very clearly points to 878p.</p>
<p>As a sidenote,
it’s important to keep in mind that this script
can’t find native 1080p elements.
This is because it descales the frame
and re-upscales it afterwards
to determine the relative error.
You can’t descale to 1080p
if the frame is already 1080p.
If you have reason to believe
that your show might be native 1080p,
you’ve got to go with your gut.</p>
<p><img src="encoding/images/descale_native1080_graph.png" alt="" />
<em>Date A Live III — 01 (getnative graph)</em></p>
<p>An example of a graph for a native 1080p show.</p>
<p>You may notice that the line swerves a bit
in the first graph.
There are going to be cases where
you will get odd graphs like these,
so it’s important to know
when the results are safe enough to descale
or when they’re too risky.
Here is an example of
a “bad” graph:</p>
<p><img src="encoding/images/descale_bad_graph1.png" alt="" />
<em>Miru Tights — 02 (getnative graph)</em></p>
<pre><code>Output:
Kernel: bicubic AR: 1.78 B: 0.33 C: 0.33
Native resolution(s) (best guess): 869p, 848p
</code></pre>
<p>The script has determined that
it’s likely either 848p
or 869p.
However,
there are no clear spikes in this graph
like there was in the Manaria Friends one.
The results are not clear enough
to work off of.
Here’s another example:</p>
<p><img src="encoding/images/descale_bad_graph2.png" alt="" />
<em>Black Lagoon (getnative graph)</em></p>
<pre><code>Output:
Kernel: bicubic AR: 1.78 B: 0.33 C: 0.33
Native resolution(s) (best guess): 1000p, 974p, 810p
</code></pre>
<p>This graph has a lot of unnatural swerves
and it’s impossible to determine
what the native resolution is.</p>
<p>Another pitfall you’ve got
to watch out for
is checking the results of a frame
with letterboxing.</p>
<p><img src="encoding/images/descale_ararararagi.png" alt="" />
<em>Kizumonogatari I</em></p>
<p><img src="encoding/images/descale_ararararagi_graph.png" alt="" />
<em>Kizumonogatari I (getnative graph)</em></p>
<p>You will have to crop them beforehand
or they will return odd graphs like this.</p>
<p>For a change of pace,
let’s look at a good graph.</p>
<p><img src="encoding/images/descale_good_graph.png" alt="" />
<em>Aikatsu Friends! — NCOP (getnative graph)</em></p>
<pre><code>Output:
Kernel: bicubic AR: 1.78 B: 0.33 C: 0.33
Native resolution(s) (best guess): 810p
</code></pre>
<p>The results are very clear.
There are a couple of deviations,
but there’s a very clear spike
going down to 810p.
This is a good result
for testing out varying kernels.</p>
<h2><a class="header" href="#descaling-1" id="descaling-1">Descaling</a></h2>
<p>Now it’s time to actually start descaling.
Open up your VapourSynth editor of choice,
and import the clip:</p>
<pre><code class="language-Py">src = core.lsmas.LWLibavSource(&quot;BDMV/[BDMV][190302][マナリアフレンズ I]/BD/BDMV/STREAM/00007.m2ts&quot;)
</code></pre>
<p>The next issue is figuring out
what was used to <a href="encoding/resampling.html#upsampling">upscale</a> the show.
By default,
getnative.py checks with Mitchell-Netravali
(bicubic b=1/3, c=1/3).
However, it might have also been upscaled
using other kernels.</p>
<p>Here is a list
of some common kernels and values.</p>
<ul>
<li>Lanczos</li>
<li>Spline16</li>
<li>Spline36</li>
<li>Bilinear</li>
<li>Bicubic b=1, c=0 (B-Spline)</li>
<li>Bicubic b=0, c=0 (Hermite)</li>
<li>Bicubic b=1/3, c=1/3 (Mitchell-Netravali)</li>
<li>Bicubic b=0, c=0.5 (Catmull-Rom)</li>
<li>Bicubic b=0, c=1 (Sharp Bicubic)</li>
</ul>
<p>The best way to figure out
what is used is to simply try out
a bunch of different kernels
and use your eyes.
Check for common scaling-related artifacting,
like haloing, ringing, aliasing, etc.</p>
<p>For bicubic,
it is important
to keep in mind that
you will typically find that
the values match the following mathematical expressions:</p>
<ul>
<li><code>b + 2c = 1</code></li>
<li><code>b = 0, c = X</code></li>
<li><code>b = 1, c = 0</code></li>
</ul>
<p>Whilst this isn’t a 100% guarantee,
it is the most common approach
to rescaling using bicubic,
so it’s worth keeping in mind.</p>
<p>Here’s an example of the previous frame
when descaled using various kernels and settings
(note that descale requires either GrayS, RGBS, or YUV444PS.
I’ll be using <code>split</code> and <code>join</code> from <code>kagefunc</code> to split the planes
and then join them again in this example,
and <code>get_w</code> from <code>vsutil</code> to calculate the width)<sup class="footnote-reference"><a href="#1">1</a></sup>:</p>
<p><a href="https://slowpics.org/comparison/61e39e1e-d074-4d83-b7c9-b0f4e1861855">Comparison between frames</a></p>
<pre><code class="language-py">from vapoursynth import core
import vsutil
import kagefunc as kgf
import fvsfunc as fvf

src = core.lsmas.LWLibavSource(&quot;BDMV/[BDMV][190302][マナリアフレンズ I]/BD/BDMV/STREAM/00007.m2ts&quot;)
src = fvf.Depth(src, 32)

y, u, v = kgf.split(src)
height = 878
width = vsutil.get_w(height)

# Bilinear
descale_a = core.descale.Debilinear(y, width, height).resize.Bilinear(1920, 1080)
descale_a = kgf.join([descale_a, u, v])
# Mitchell-Netravali
descale_b = core.descale.Debicubic(y, width, height, b=1/3, c=1/3).resize.Bicubic(1920, 1080, filter_param_a=1/3, filter_param_b=1/3)
descale_b = kgf.join([descale_b, u, v])
# Sharp Bicubic
descale_c = core.descale.Debicubic(y, width, height, b=0, c=1).resize.Bicubic(1920, 1080, filter_param_a=0, filter_param_b=1)
descale_c = kgf.join([descale_c, u, v])
# B-Spline
descale_d = core.descale.Debicubic(y, width, height, b=1, c=0).resize.Bicubic(1920, 1080, filter_param_a=1, filter_param_b=0)
descale_d = kgf.join([descale_d, u, v])
# Catmull-rom
descale_e = core.descale.Debicubic(y, width, height, b=0, c=1/2).resize.Bicubic(1920, 1080, filter_param_a=0, filter_param_b=1/2)
descale_e = kgf.join([descale_e, u, v])
# Spline36
descale_f = core.descale.Despline36(y, width, height).resize.Spline36(1920, 1080)
descale_f = kgf.join([descale_f, u, v])
</code></pre>
<p>You might realize that after descaling,
we are immediately upscaling the frame
with the same kernel and values again.
This is done so we can compare the before and after.
The closer the new frame is to the old one,
the more likely it is that you’ve got the correct kernel.
Zooming in on the frame at 4x magnification or higher
using Nearest Neighbor
will help immensely.
An alternative that you can use
is to simply descale until
you’ve got what you believe to be the best result.
It’s faster to do it this way,
but might be less accurate.</p>
<h2><a class="header" href="#credits-and-other-native-1080p-elements" id="credits-and-other-native-1080p-elements">Credits and other native 1080p elements</a></h2>
<p>There is one very, very important thing
to keep in mind when descaling:</p>
<p><em>Credits are usually done in 1080p</em>.</p>
<p>There are various masks you can use
to help with dealing with that issue,
but it might be better
to make use of existing wrappers instead.
For this example I’ll
be using <code>inverse_scale</code> from <code>kagefunc</code>.</p>
<pre><code class="language-Py">descaled = kgf.inverse_scale(src, height=878, kernel='bicubic', b=0, c=1/2, mask_detail=True)
</code></pre>
<p>We can make use of the mask
that <code>inverse_scale</code> uses internally
as well.</p>
<pre><code class="language-Py">descaled = kgf.inverse_scale(src, height=874, kernel='bicubic', b=0, c=1/2)
descaled_mask = kgf._generate_descale_mask(vsutil.get_y(core.resize.Spline36(src, descaled.width, descaled.height)), vsutil.get_y(descaled), kernel='bicubic', b=0, c=1/2)
</code></pre>
<p><img src="encoding/images/descale_credits_mask.png" alt="" />
<em>Kaguya-sama: Love Is War — OP (credits mask)</em></p>
<p><img src="encoding/images/descale_credits.png" alt="" />
<em>Kaguya-sama: Love Is War — OP (descaled)</em></p>
<p>Note that if you see the mask
catching a lot of other stuff,
you might want to consider <em>not</em> descaling
that particular frame,
or trying a different kernel/values.
Chances are that you’re either
using the wrong kernel
or that the frames you’re looking at are native 1080p.</p>
<p><img src="encoding/images/descale_native1080.png" alt="" />
<em>Manaria Friends — 01 (end card)</em></p>
<p><img src="encoding/images/descale_dontdescalethis.png" alt="" />
<em>(Don’t descale this)</em></p>
<h2><a class="header" href="#dealing-with-bad-descaling" id="dealing-with-bad-descaling">Dealing with bad descaling</a></h2>
<p>There are various things you can do
to deal with scenes that have issues
even after descaling.
<code>Eedi3</code> stands out in particular
as a fantastic AA filter
that really nails down bad lineart
caused by bad descaling.
It’s at best a “cheat code”,
however.
While it might fix up some issues,
it won’t fix everything.
It’s also incredibly slow,
so you might want to
<a href="encoding/scenefiltering.html">use it on just a couple of frames at a time</a>
rather than over the entire clip.</p>
<p>Other than Eedi3,
usually the results of bad descaling
are so destructive that
there isn’t much you can do.
If you have an encode that’s
badly descaled,
you’re better off
finding a different one.
If you’ve got bad results
after descaling yourself,
try a different kernel or values.
Alternatively,
try not descaling at all.</p>
<p>At the end of the day,
as mentioned in the introduction,
you can’t descale everything perfectly.
Sometimes it’s better to think of it
as a magical anti-ringing/haloing/aliasing filter
rather than a scaler.</p>
<p>For example,
here it was used
specifically to fix up
some bad aliasing
in the source.</p>
<p><img src="encoding/images/descale_akanesasu_src.png" alt="" />
<em>Akanesasu Shoujo — 01 (src)</em></p>
<p><img src="encoding/images/descale_akanesasu_rescaled.png" alt="" />
<em>Akanesasu Shoujo — 01 (rescaled)</em></p>
<pre><code class="language-py">scaled = kgf.inverse_scale(src, height=900, kernel='bicubic', b=0.25, c=0.45, mask_detail=True)
scaled = nnedi3_rpow2(scaled).resize.Spline36(1920, 1080)
</code></pre>
<p>Note how this fixed
most of the aliasing
on the CGI model.</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Most, if not all, relevant VapourSynth scripts/plug-ins and their functions can be found in the <a href="http://vsdb.top/">VapourSynth Database</a>.</p>
</div>
<h1><a class="header" href="#resampling" id="resampling">Resampling</a></h1>
<p>Resampling is a technique applied in various
image processing tasks,
most prominently scaling/resizing,
but also shifting and rotation.</p>
<h2><a class="header" href="#resizing" id="resizing">Resizing</a></h2>
<p>The most common class of resampling algorithms
used for resizing are the convolution-based resamplers.
As the name suggests,
these work by <a href="https://en.wikipedia.org/wiki/Convolution">convolving</a> the image
with a filter function.
These filter functions,
also known as kernels,
are what the terms Bilinear/Bicubic/Lanczos/etc. generally refer to.
The <a href="https://entropymine.com/imageworsener/resample/">ImageWorsener documentation</a> features a great
visual explanation of how this process works in detail.
It is strongly recommended to read it.
If you wish to explore the mathematics behind the design
of these filters,
the <a href="https://www.pbrt.org/chapters/pbrt_chapter7.pdf">Sampling and Reconstruction chapter</a>
of <a href="https://www.pbrt.org/">Physically Based Rendering</a> is one of the best places
for a layman to start.</p>
<h3><a class="header" href="#filters" id="filters">Filters</a></h3>
<p>All resampling kernels behave slightly differently
and generate artifacts of differing kinds and severity.</p>
<p>It should be noted that there is no “objectively best” resampling filter,
so it is largely a matter of personal preference.
There are no hard and fast rules about
which resampler performs best for any given type of content,
so it’s best to test them yourself.</p>
<p>A short overview of the most common filters follows.
For a much more extensive explanation of the different filters,
including details on the exact algorithms,
see <a href="http://www.imagemagick.org/Usage/filter/">ImageMagick’s guide</a>.</p>
<p>Additionally,
<a href="https://forum.doom9.org/showthread.php?t=160038">ResampleHQ</a>‘s documentation features
an excellent visual comparison of common filter kernels;
a back-up is available <a href="http://maven.whatbox.ca:11665/resample_kernels/kernels.html">here</a>.</p>
<h4><a class="header" href="#box-filter--nearest-neighbour" id="box-filter--nearest-neighbour">Box filter / Nearest Neighbour</a></h4>
<p>When upscaling,
the Box filter will behave just like
<a href="https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation">Nearest Neighbour (<em>NN</em>) interpolation</a>,
that is,
it will just pick the closest pixel in the input image
for every output pixel.
This results in the source pixel grid being magnified
without any smoothing or merging of adjacent pixels,
providing a faithful representation of the original pixel grid.
This is very useful when inspecting an image up close
to examine the pixel structure,
or when enlarging <a href="https://en.wikipedia.org/wiki/Pixel_art">pixel art</a>,
but not suitable for regular content
due to the jagged lines and deformed details it causes.</p>
<p>When downscaling,
the Box filter behaves differently to
NN—which continues to just
pick the closest pixel and
be done with it—in that it instead
merges adjacent pixels together.
(This is because generally,
filter kernels are widened
in proportion to the scaling factor
when downscaling,
which, in effect, applies a <a href="https://en.wikipedia.org/wiki/Low-pass_filter">low-pass filter</a>
that serves to prevent aliasing.)
Unlike most other filters, however,
it averages them evenly
instead of giving the central ones more weight.
(For example, reducing a 10 pixel row to 5 pixels
will average every pixel pair.)
This, again,
can be a useful property in specific cases,
but is not generally desirable.</p>
<p>The Box filter is available in VapourSynth
in the <code>fmtconv</code> plug-in:</p>
<pre><code class="language-py">clip = core.fmtc.resample(src, w, h, kernel=&quot;box&quot;)
</code></pre>
<p>Nearest Neighbour interpolation is part of the built-in <code>resize</code> plug-in:</p>
<pre><code class="language-py">clip = core.resize.Point(src, w, h)
</code></pre>
<p>Most <a href="encoding/preparation.html#the-editor">script editors</a> including VSEdit feature NN scaling in the preview;
it is recommended to use it over Bilinear
when making filtering decisions.</p>
<h4><a class="header" href="#bilinear--triangle" id="bilinear--triangle">Bilinear / Triangle</a></h4>
<p>Bilinear, also known as Triangle due to its graph’s shape,
is one of the most common algorithms in widespread use
because of its simplicity and speed.
However,
it generally creates all sorts of nasty artifacts
and is inferior in quality to most other filters.
The only advantage it offers is speed,
so don’t use it unless you’re sure you have to.</p>
<p>VapourSynth example:</p>
<pre><code class="language-py">clip = core.resize.Bilinear(src, w, h)
</code></pre>
<h4><a class="header" href="#mitchell-netravali--bicubic" id="mitchell-netravali--bicubic">Mitchell-Netravali / Bicubic</a></h4>
<p>The Mitchell-Netravali filter,
also known as Bicubic,
is one of the most popular resampling algorithms,
and the default for many image processing programs,
because it is usually considered a good neutral default.</p>
<p>It takes two parameters, B and C,
which can be used to tweak the filter’s behaviour.
For upscaling, it is recommended to use values that satisfy the equation
\(\mathrm{b} + 2\mathrm{c} = 1\).</p>
<p>The graph below outlines
the various kinds of artifacts
different B-C-configurations produce.</p>
<p><img src="encoding/images/resample_cubic_survey.gif" alt="" />
<em>Bicubic B-C parameters</em></p>
<p>Roughly speaking,
raising B will cause blurring
and raising C will cause ringing.</p>
<p>Mitchell-Netravali generalizes all smoothly fitting
(continuous first derivative) piece-wise cubic filters,
so any of them can be expressed with the appropriate parameters.
Below you can find a list of common cubic filters
and their corresponding parameters in Mitchell-Netravali.</p>
<ul>
<li>B-Spline – b=1, c=0</li>
<li>Hermite – b=0, c=0</li>
<li>Mitchell-Netravali – b=1/3, c=1/3 (sometimes referred to as just “Mitchell”)</li>
<li>Catmull-Rom – b=0, c=0.5</li>
<li>Sharp Bicubic – b=0, c=1</li>
</ul>
<p>Hermite is often considered one of the best choices for downscaling,
as it produces only minimal artifacting,
at the cost of slight blurriness.</p>
<p>VapourSynth example:</p>
<pre><code class="language-py"># 'filter_param_a' and 'filter_param_b' refer to B and C, respectively
clip = core.resize.Bicubic(src, w, h, filter_param_a=0, filter_param_b=0.5)
</code></pre>
<h4><a class="header" href="#lanczos" id="lanczos">Lanczos</a></h4>
<p>Lanczos is generally considered
a very high-quality resampler for upscaling,
especially its <a href="encoding/resampling.html#elliptical-weighted-averaging-ewa-cylindrical-polar-circular">EWA version</a>.</p>
<p>It is usually slightly sharper than Mitchell (Bicubic b=c=1/3),
but might produce slightly more ringing.</p>
<p>Lanczos takes an additional parameter
that controls the filter’s number of lobes,
or <em>taps</em>.
Increasing the number of lobes
improves sharpness at the cost of increased ringing.
You might occasionally see the tap count
appended to the filter name to clarify
the exact filter used, e.g. Lanczos2 for 2 taps.</p>
<p>For downscaling,
higher tap counts might help in
suppressing <a href="http://www.imagemagick.org/Usage/filter/#aliasing">Moiré effects</a>.</p>
<pre><code class="language-py"># 'filter_param_a' specifies the tap count
clip = core.resize.Lanczos(src, w, h, filter_param_a=2)
</code></pre>
<h4><a class="header" href="#spline" id="spline">Spline</a></h4>
<p>Spline is another high-quality resizer.</p>
<p>Spline, like Lanczos,
can be fine-tuned by configuring its number of lobes.
Unlike Lanczos,
however,
Splines with different tap counts are usually split
into separate functions,
with \((\mathrm{tap~count} \times 2)^2\) appended to their name,
e.g. Spline36 for 3 taps, Spline64 for 4, etc.
(This number represents the total amount of input pixels
involved in the calculation of any given output pixel.)</p>
<p>Spline36 is a very popular choice for downscaling,
since it is fairly artifact-free yet decently sharp.
For upscaling,
it looks similar to Lanczos3,
though arguably slightly less artifacted.</p>
<p>VS example:</p>
<pre><code class="language-py">clip = core.resize.Spline36(src, w, h)
</code></pre>
<p>Higher tap counts can be used via fmtconv:</p>
<pre><code class="language-py">clip = core.fmtc.resample(src, w, h, kernel=&quot;spline&quot;, taps=6) # equivalent to Spline144
</code></pre>
<h4><a class="header" href="#gauss" id="gauss">Gauss</a></h4>
<p>The Gaussian filter is very special in that its Fourier transform<sup class="footnote-reference"><a href="#1">1</a></sup>
is another Gaussian
whose width is inversely proportional to the spatial function’s.
This can be harnessed to remove and amplify high frequencies
in a very controllable way.
Widening the filter,
for example,
will confine the output to small frequencies (blurrier),
whereas squashing it will amplify higher frequencies (more aliasing).</p>
<p>In practice,
though,
the Gaussian filter isn’t all too useful for regular resizing.
However,
it can be used to accurately emulate a Gaussian blur
(when used without resizing)
in VapourSynth.</p>
<p>For example:</p>
<pre><code class="language-py">blurred = core.fmtc.resample(src, kernel=&quot;gauss&quot;, fh=-1, fv=-1, a1=1)
</code></pre>
<p><code>fh=-1, fv=-1</code> forces the processing
when no resizing is performed.
<code>a1</code> controls the blurring:
the higher,
the sharper the image.</p>
<h3><a class="header" href="#interpolation-filters" id="interpolation-filters">Interpolation filters</a></h3>
<p>Some sources will categorize filters as either
interpolation filters or non-interpolation filters.</p>
<p>Interpolation filters are those that
when applied “in-place”,
i.e. at the location of the input samples,
don’t alter the sample value.
Therefore,
they only interpolate “missing” values,
leaving the input samples untouched.</p>
<p>This is true for filters that evaluate to 0
at all integer positions except 0
or whose support is &lt;= 1.
Examples include:
(Windowed) Sinc filters,
such as Lanczos,
Bicubic with B=0,
e.g. Hermite and Catmull-Rom,
and Triangle/Bilinear.</p>
<p>This can be a beneficial property in some cases,
for example the No-Op case.
No-Op means that no scaling, shifting or similar is performed,
that is, the input is resampled at exactly the same positions.
In this case,
an interpolation filter will return the input image untouched,
whereas other filters will slightly alter it.</p>
<p>Another,
more practical,
case where this property is useful
is when shifting an image by full pixel widths (integers),
again because input pixel values aren’t changed
but just relocated.</p>
<h3><a class="header" href="#two-dimensional-resampling" id="two-dimensional-resampling">Two-dimensional resampling</a></h3>
<p>There are two different ways to go about
resampling in two dimensions.</p>
<h4><a class="header" href="#tensor-resampling-orthogonal-2-pass-separated" id="tensor-resampling-orthogonal-2-pass-separated">Tensor resampling (<em>orthogonal</em>, <em>2-pass</em>, <em>separated</em>)</a></h4>
<p>The image is resampled in two separate passes:
First it is resampled horizontally, then vertically.
This allows images to be treated 1-dimensionally
since each pixel row/column can be resampled separately.
The main advantage of this method is that it’s extremely fast,
which is why it’s the much more common one;
generally, unless indicated otherwise,
this is what is used.</p>
<h4><a class="header" href="#elliptical-weighted-averaging-ewa-cylindrical-polar-circular" id="elliptical-weighted-averaging-ewa-cylindrical-polar-circular">Elliptical Weighted Averaging (<em>“EWA”</em>, <em>cylindrical</em>, <em>polar</em>, <em>circular</em>)</a></h4>
<p><img src="encoding/images/resample_polar.png" alt="" />
<em>Two-dimensional kernel. The radius is colored green.</em></p>
<p>All input samples whose <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> to the pixel
is within the filter’s radius contribute to its value.
The Euclidean distance is passed to the filter kernel.
This is a lot more costly than tensor resampling in terms of runtime.</p>
<h3><a class="header" href="#scaling-in-modified-colorspaces" id="scaling-in-modified-colorspaces">Scaling in modified colorspaces</a></h3>
<p>The colorspace used when resampling
can significantly impact the output’s subjective quality.</p>
<h4><a class="header" href="#downscaling-in-linear-light" id="downscaling-in-linear-light">Downscaling in linear light</a></h4>
<p>Downscaling in gamma-corrected light instead of linear light
can sometimes noticeably dim the image.
To see why this happens,
consider this gradient:</p>
<p><img src="encoding/images/resample_gamma_shades.jpg" alt="" />
<em>A grayscale gradient from 0 to 255.</em></p>
<p>It should be apparent
that the brightness doesn’t scale linearly with the pixel values.
This is because most digital video
uses <a href="https://en.wikipedia.org/wiki/Gamma_correction">gamma-transformed</a> pixel values
in order to compress more perceptually distinct color shades
into 8 bit.
This causes the encoded pixel values
to deviate from their expected brightness,
e.g. a grey pixel has value 187 instead of 127 in sRGB.
This poses a problem when merging and interpolating colors,
because the average pixel value of two colors no longer corresponds
to their average perceived brightness.
For example,
if you wanted to merge black and white (0 and 255),
you would expect to get grey,
but since grey actually has a value of ~187,
the output pixel would turn out substantially darker,
if you were you to naively average the pixel values.</p>
<p>To calculate the correct values,
the gamma transform needs to be reversed before scaling
and re-applied afterwards.</p>
<p>The dimming effect of scaling in gamma-corrected light
is usually only noticeable
in dense color patterns,
e.g. small black text on a white background,
stars in the night sky, etc,
and much less so in blurrier areas.</p>
<p>See <a href="https://slowpics.org/comparison/8103b2d1-b9d4-4d7e-b7a7-3197ae999244">this comparison</a> for a particularly extreme example
of linear vs gamma downscaling.</p>
<p>However,
this doesn’t necessarily mean downscaling in linear light
is always the right choice,
since it noticeably accentuates dark halos
introduced by scaling.
Thus,
it may be wise to scale in gamma light
when using a resizer prone to overshooting,
like high-lobe Lanczos.
Besides,
the dimming may even be desirable in some cases like black text on white paper,
because it preserves legibility.</p>
<p>If you choose to downscale in linear light,
make sure to use a sufficiently high bitdepth
so as to not introduce banding.</p>
<p>Example code for resizing in linear RGB light:</p>
<pre><code class="language-py">linear = core.resize.Bicubic(src, format=vs.RGBS, transfer_in_s=&quot;709&quot;, transfer_s=&quot;linear&quot;, matrix_in_s=&quot;709&quot;)
scaled_linear = core.resize.Bicubic(linear, 640, 360)
scaled_gamma = core.resize.Bicubic(scaled_linear, format=src.format, transfer_s=&quot;709&quot;, transfer_in_s=&quot;linear&quot;, matrix_s=&quot;709&quot;)
</code></pre>
<p>Note that the <code>matrix_s</code> and <code>matrix_in_s</code> arguments
are only necessary when <code>src</code> is YUV;
otherwise, they should be omitted.</p>
<h4><a class="header" href="#upscaling-in-sigmoidized-light" id="upscaling-in-sigmoidized-light">Upscaling in sigmoidized light</a></h4>
<p>In order to attenuate both dark and white halos
introduced by upscaling,
you can resize through a sigmoidized colorspace.</p>
<p>This means converting the linear RGB version of an image
to a custom colorspace with an S-shaped intensity curve
before scaling and converting it back afterwards.
What this does, essentially,
is decrease the image’s contrast
by pushing extreme values of both dark and bright
towards the middle.</p>
<p>Quoting Nicholas Robidoux from ImageMagick<sup class="footnote-reference"><a href="#2">2</a></sup>:</p>
<blockquote>
<p>You may decrease halos and increase perceptual sharpness by increasing the sigmoidal contrast (up to 11.5, say).
Higher contrasts are especially recommended with greyscale images (even “false RGB greyscale” that have three proportional color channels).
The downside of sigmoidization is that it sometimes produces “color bleed” artefacts that look a bit like cheap flexographic (”gummidruck”) printing or chromatic aberration.
In addition, sigmoidization’s “fattening” of extreme light and dark values may not work for your image content.
If such artefacts are obvious, push the contrast value down from 7.5 (to 5, for example, or even lower).
Setting the contrast to 0 is equivalent to enlarging through linear RGB. <em>(Robidoux, 2012)</em></p>
</blockquote>
<p>Example code for VS:</p>
<pre><code class="language-py">import havsfunc as hf
linear = core.resize.Bicubic(src, format=vs.RGBS, transfer_in_s=&quot;709&quot;, transfer_s=&quot;linear&quot;, matrix_in_s=&quot;709&quot;)
sigmoidized = hf.SigmoidInverse(linear, thr=0.5, cont=6.5) # 'cont' corresponds to the “sigmoidal contrast” mentioned above
scaled_sigmoid = core.resize.Bicubic(sigmoidized, 640, 360)
de_sigmoidized = hf.SigmoidDirect(scaled_sigmoid, thr=0.5, cont=6.5)
scaled_gamma = core.resize.Bicubic(de_sigmoidized, format=src.format, transfer_s=&quot;709&quot;, transfer_in_s=&quot;linear&quot;, matrix_s=&quot;709&quot;)
</code></pre>
<h3><a class="header" href="#neural-network-scalers" id="neural-network-scalers">Neural network scalers</a></h3>
<p>NN-based scalers have become
increasingly popular in recent times.
This is because
they aren’t subject to the technical limitations
of convolution-based resamplers—which beyond a certain point
only trade one artifact for another—and thus produce
much higher quality upscales.</p>
<h5><a class="header" href="#nnedi3" id="nnedi3">NNEDI3</a></h5>
<p>This is the current de-facto standard
for high-quality upscaling
because it generally produces equally as sharp or sharper
images than conventional scalers,
while avoiding any major artifacting
such as haloing, ringing or aliasing.</p>
<p>Nnedi3 was originally conceived as a deinterlacer;
as such,
it only doubles a frame’s height,
leaving the original pixel rows untouched
and interpolating the missing ones.
This, however, can trivially be leveraged
to increase image dimensions by powers of two
(by doubling <em>n</em> times,
flipping the image,
doubling <em>n</em> times again,
and flipping back).</p>
<p>Upsampling to arbitrary dimensions can be achieved
by scaling by the smallest power of two
that results in a bigger image than desired,
and downscaling to the requested resolution
with a conventional scaler
(the most popular choice for this is <a href="encoding/resampling.html#spline">Spline36</a>).</p>
<p>However,
you should note that
good quality comes at a cost:
nnedi3 will generally be several orders of magnitude
slower than conventional resamplers.</p>
<p>VapourSynth usage example:</p>
<pre><code class="language-py">from nnedi3_rpow2 import *
# 'spline36' here is technically redundant since it’s the default
up = nnedi3_rpow2(src, width=1920, height=1080, kernel=&quot;spline36&quot;)
</code></pre>
<h2><a class="header" href="#shifting" id="shifting">Shifting</a></h2>
<p>Shifting an image by an arbitrary amount,
including non-integer values,
requires resampling as well.
For example,
left-shifting by a quarter pixel
will resample the image at the input samples’ positions minus 0.25.<sup class="footnote-reference"><a href="#3">3</a></sup>
This also means that,
unless a <a href="encoding/resampling.html#interpolation-filters">interpolative filter</a> is used,
even shifting by integer amounts will alter the image.</p>
<p>Side note:
It can be interesting to think of shifting
not as resampling at shifted pixel locations,
but as resampling at the input locations
with a shifted kernel.</p>
<h3><a class="header" href="#chroma-shifting" id="chroma-shifting">Chroma shifting</a></h3>
<p>When going from 4:2:0 <a href="https://en.wikipedia.org/wiki/Chroma_subsampling">subsampling</a>
to 4:4:4 (no subsampling),
it is important to take into account chroma placement
and to shift the chroma accordingly
to ensure it aligns with the luma.</p>
<p><img src="encoding/images/resample_chroma_placement.png" alt="" />
<em>YUV 4:2:0 subsampling with center-aligned chroma (left) and, as per MPEG-2, left-aligned chroma (right).</em></p>
<p>There are two commonly used
chroma siting patterns,
as illustrated by the graphic above.
Most digital video today
uses the MPEG-2 variant,
that is,
left-aligned chroma.
This is essential to keep in mind
when going from 4:2:0 to 4:4:4,
because if the chroma planes
are naively upscaled and joined with the luma plane without any shifting,
the chroma will be shifted by a quarter pixel.
This is a consequence of the way
output pixels are usually mapped onto the input grid
during resampling:</p>
<p><img src="encoding/images/resample_matchedges.png" alt="" />
<em>Pixel mapping in common resampling algorithms (2 -&gt; 4 upscale).</em></p>
<p>Essentially,
the output grid is scaled
such that the outer edges of the pixel boxes align,
importantly under the assumption that samples are center-aligned
within the pixel boxes.
Therefore,
when scaling a chroma plane by 200%,
which is what happens to the chroma
when going from 4:2:0 to 4:4:4,
the new chroma sample positions will
match up with the luma sample positions.
This would be the correct mapping
if the resamplers’s assumption of center-alignment was true—if it isn’t
(like with MPEG-2 chroma placement)
we have to compensate for the offset by shifting the input samples
by a quarter pixel width to the left
before calculating the output samples.
This way,
the left-alignment is restored.</p>
<p>Similarly,
when resizing left-aligned 4:2:0 material
while keeping the subsampling,
a slight shift needs to be applied
to preserve the alignment.
Specifically,
the chroma needs to be shifted by
\(0.25 - 0.25 \times \frac{\mathrm{src~width}}{\mathrm{dst~width}}\).<sup class="footnote-reference"><a href="#4">4</a></sup></p>
<p>Chroma shifting is performed
automatically under the hood by most format conversion software
(including zimg, VapourSynth’s resizing library)
and media players.
Thus, we only need to take care of it
if we handle the chroma upscaling separately by hand.</p>
<p>In VS,
shifting can be performed with the <code>resize</code> functions’ <code>src_left</code> parameter:</p>
<pre><code class="language-py">u = core.std.ShufflePlanes(src, planes=1, colorfamily=vs.GRAY)
shifted_scaled_u = core.resize.Spline16(u, 1920, 1080, src_left=0.25) # shifts the image to the left by 0.25 pixels
</code></pre>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>The Fourier transform is an ubiquitous concept in image processing, so we strongly advise becoming familiar with at least the basics. A very good resource for this topic is <a href="http://www.fmwconcepts.com/imagemagick/fourier_transforms/fourier.html">ImageMagick’s guide</a>.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Robidoux, N. (2012, October 21). Resampling — ImageMagick v6 Examples. Retrieved August 22, 2019, from https://www.imagemagick.org/Usage/filter/nicolas/#upsampling</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>If you don’t understand what this means, read the resources linked above in the <a href="encoding/resampling.html#resizing">resizing section</a>.</p>
</div>
<div class="footnote-definition" id="4"><sup class="footnote-definition-label">4</sup>
<p>This is derived as follows: The shift is the distance between the position of the first luma sample and the position of the first chroma sample (both mapped onto the input grid and given in terms of input chroma pixel widths). The former is located at \(0.25 + \frac{\mathrm{src~width}}{4 \times \mathrm{dst~width}}\), the latter at \(\frac{\mathrm{src~width}}{2 \times \mathrm{dst~width}}\). This yields \(0.25 + \frac{\mathrm{src~width}}{4 \times \mathrm{dst~width}} - \frac{\mathrm{src~width}}{2 \times \mathrm{dst~width}} = 0.25 + \frac{\mathrm{src~width}}{\mathrm{dst~width}} \times \left( ^1/_4 -,^1/_2 \right) = 0.25 + \frac{\mathrm{src~width}}{\mathrm{dst~width}} \times (-0.25)\) for the shift.</p>
</div>
<h1><a class="header" href="#encoding-with-x264" id="encoding-with-x264">Encoding with x264</a></h1>
<p>H.264 has been the de facto standard video format
across the internet for the past decade.
It is widely supported for playback in all modern browsers
and many hardware devices such as gaming consoles and phones.
It provides better video quality at smaller file sizes
compared to its predecessors.</p>
<p>x264 is a mature, free, open-source encoder
for the H.264 video format.</p>
<h2><a class="header" href="#prerequisites" id="prerequisites">Prerequisites</a></h2>
<p>To get started, you’ll need two things:</p>
<ul>
<li>A video to encode—for the examples,
we will pipe in a video from VapourSynth,
which you should be able to do
if you’ve been following the previous sections of this guide</li>
<li>The x264 encoder</li>
</ul>
<p>Here’s how we get a copy of the x264 encoder:</p>
<h3><a class="header" href="#windows" id="windows">Windows</a></h3>
<p>Official Windows builds are available
<a href="https://artifacts.videolan.org/x264/release-win64/">here</a>.</p>
<h3><a class="header" href="#linuxmacos" id="linuxmacos">Linux/macOS</a></h3>
<p>Generally, x264 will be available
through your distribution’s package manager.
Here are a few examples:</p>
<ul>
<li><strong>Ubuntu/Debian</strong>: <code>sudo apt install x264</code></li>
<li><strong>Arch Linux</strong>: <code>sudo pacman -S x264</code></li>
<li><strong>macOS</strong>: <code>brew install x264</code></li>
</ul>
<h2><a class="header" href="#getting-started" id="getting-started">Getting Started</a></h2>
<p>x264 is very configurable,
and the options may seem overwhelming.
But you can get started encoding
by using the presets x264 provides
and understanding a few basic concepts.
We’ll walk through those concepts
with the following examples.</p>
<h3><a class="header" href="#example-1-general-purpose-encoding" id="example-1-general-purpose-encoding">Example 1: General-Purpose Encoding</a></h3>
<p>Open up a terminal window,
and navigate to the folder
where your VapourSynth script lives.
Let’s run the following command:</p>
<pre><code>vspipe --y4m myvideo.vpy - | x264 --demuxer y4m --preset veryfast --tune animation --crf 24 -o x264output.mkv -
</code></pre>
<p>Let’s run through what each of these options means:</p>
<h5><a class="header" href="#vspipe---y4m-myvideovpy--" id="vspipe---y4m-myvideovpy--"><code>vspipe --y4m myvideo.vpy -</code></a></h5>
<p>This portion loads your VapourSynth script
and pipes it to stdout,
adding y4m headers that x264 can decode.
If you use Linux,
you’re probably familiar with how piping works.
If you’re not,
it’s basically a way of chaining two commands together.
In this case, we want to chain <code>vspipe</code>,
the program that reads VapourSynth scripts,
with <code>x264</code>, our encoder.</p>
<h5><a class="header" href="#--demuxer-y4m" id="--demuxer-y4m"><code>--demuxer y4m</code></a></h5>
<p>This tells x264 that we’re providing it with a y4m file.
This matches up with the <code>--y4m</code> flag
that we gave to the <code>vspipe</code> command.</p>
<h5><a class="header" href="#--preset-veryfast" id="--preset-veryfast"><code>--preset veryfast</code></a></h5>
<p>x264 has a set of presets
to switch between faster encoding, or higher quality.
The full list of presets, from fastest to slowest, is:</p>
<ol>
<li>ultrafast</li>
<li>superfast</li>
<li>veryfast</li>
<li>faster</li>
<li>fast</li>
<li>medium</li>
<li>slow</li>
<li>slower</li>
<li>veryslow</li>
<li>placebo</li>
</ol>
<p>You will almost never want to use the extreme settings,
but generally, if you want good quality
and don’t care about how long the encode takes,
<code>slower</code> or <code>veryslow</code> are recommended.
In this example,
because we are just demonstrating how x264 works,
we want a fast encode and have chosen <code>veryfast</code>.</p>
<p>For the curious,
you can see a full list of the settings enabled by each preset
by running <code>x264 --fullhelp | less</code> (Linux/Mac)
or <code>x264 --fullhelp | more</code> (Windows).
However, this probably won’t mean much at the moment.
Don’t worry,
this page will explain later
what all of those settings mean.</p>
<p><em>Disclaimer:
x264’s <code>fullhelp</code> is not guaranteed to be up-to-date.</em></p>
<h5><a class="header" href="#--tune-animation" id="--tune-animation"><code>--tune animation</code></a></h5>
<p>Beyond the preset chosen,
x264 allows us to further tune the encoding settings
for the type of content we’re working with.
The following tunings are generally the most useful:</p>
<ul>
<li><code>film</code>: Recommended for live action videos.</li>
<li><code>animation</code>: Recommended for anime or cartoons with flat textures.
For 3D animation (e.g. Pixar movies),
you may find better results with <code>film</code>.</li>
<li><code>grain</code>: Recommended for particularly grainy films.</li>
</ul>
<p>You don’t need to use a tuning,
but it generally helps
to produce a better-looking video.</p>
<h5><a class="header" href="#--crf-24" id="--crf-24"><code>--crf 24</code></a></h5>
<p>CRF is a constant-quality, 1-pass encoding mode.
In layman’s terms,
this means that we don’t need the output to meet a specific filesize,
we just want the output to meet a certain quality level.
CRF ranges from 0 to 51 (for 8-bit encoding),
with 0 being the best quality
and 51 being the smallest filesize,
but there is a certain range of CRF settings
that are generally most useful.
Here are some guidelines:</p>
<ul>
<li>CRF 13: This is considered visually lossless to videophiles.
This can produce rather large files,
but is a good choice if you want high quality videos.
Some fansubbing groups use this for Blu-ray encodes.</li>
<li>CRF 16-18: This is considered visually lossless to most viewers,
and leans toward high quality
while still providing a reasonable filesize.
This is a typical range for fansub encodes.</li>
<li>CRF 21-24: This provides a good balance between quality and filesize.
Some quality loss becomes visible,
but this is generally a good choice
where filesize becomes a concern,
such as for videos viewed over the internet.</li>
<li>CRF 26-30: This prioritizes filesize,
and quality loss becomes more obvious.
It is generally not recommended to go higher than CRF 30
in any real-world encoding scenario,
unless you want your videos to look like they were made for dial-up.</li>
</ul>
<h5><a class="header" href="#-o-x264outputmkv--" id="-o-x264outputmkv--"><code>-o x264output.mkv -</code></a></h5>
<p>This last portion tells which files to use for the input and output.
We use <code>-o</code> to tell which filename to write the encoded file to.
In this case, x264 will write a file at <code>x264output.mkv</code>
in the current directory.</p>
<p>The last argument we are passing to x264 is the input file.
In this case, we pass <code>-</code> for the input file,
which tells x264 to use the piped output from vspipe.
The input argument is the only positional argument,
so it does not need to be last;
x264 will recognize it
as the only argument without a <code>--</code> flag before it.</p>
<h3><a class="header" href="#example-2-targeted-file-size" id="example-2-targeted-file-size">Example 2: Targeted File Size</a></h3>
<p>For the next example,
let’s say we want to make sure our encode
fits onto a single 4.7GB DVD<sup class="footnote-reference"><a href="#1">1</a></sup>.
How would we do that in x264?</p>
<p>First, we’ll need to figure out
what bitrate our encode should be,
in <strong>kilobits per second</strong>.
This means we’ll need to know a couple of things:</p>
<ul>
<li>The length of our video, in seconds.
For this example,
let’s say our movie is 2 hours (120 minutes) long.
We’ll convert that to seconds:
120 minutes * 60 minutes/second = <strong>7200 seconds</strong>.</li>
<li>Our target filesize.
We know that this is 4.7GB,
but we need to convert it to kilobits.
We can do this with the following steps:</li>
</ul>
<p>$$
\begin{aligned}
4.7~\mathrm{GB}\times \frac{1000~\mathrm{MB}}{\mathrm{GB}} &amp;= 4700~\mathrm{MB}\\
4700~\mathrm{MB}\times \frac{1000~\mathrm{KB}}{\mathrm{MB}} &amp;= 4,700,000~\mathrm{KB}\\
4,700,000~\mathrm{KB}\times \frac{8~\mathrm{Kbit}}{\mathrm{KB}} &amp;= 37,600,000~\mathrm{Kbit}
\end{aligned}
$$</p>
<p>Now we divide the kilobit size we calculated by our video length,
to find our kilobit per second target bitrate:</p>
<p>$$
37,600,000~\mathrm{Kbit}\div 7200~\mathrm{seconds} \approx 5222~\mathrm{Kbps}
$$</p>
<p>There is also a <a href="https://gist.githubusercontent.com/OrangeChannel/816a87cf760d9be19bde18db8818d4bc/raw/bitrate_filesize.py">python script</a> that can handle this calculation for us:</p>
<pre><code class="language-py">&gt;&gt;&gt; from bitrate_filesize import *
&gt;&gt;&gt; find_bitrate('4.7 GB', seconds=7200)
bitrate should be 5,222 kbps
</code></pre>
<p>And here’s how we could add that to our x264 command:</p>
<pre><code>vspipe --y4m myvideo.vpy - | x264 --demuxer y4m --preset veryfast --bitrate 5222 -o x264output.mkv -
</code></pre>
<p>The <code>--bitrate</code> option, by itself,
says that we want to do a 1-pass, average-bitrate encode.
In other words, the encoder will still give more bits
to sections of the video that have more detail or motion,
but the average bitrate of the video
will be close to what we requested.</p>
<h3><a class="header" href="#example-3-2-pass-encoding" id="example-3-2-pass-encoding">Example 3: 2-Pass Encoding</a></h3>
<p>So far, we’ve only done 1-pass encodes.
While using CRF 1-pass is great
when you don’t have a target bitrate,
it’s recommended not to use 1-pass
for targeted-bitrate encodes,
because the encoder can’t know
what’s coming ahead of the current section of video.
This means it can’t make good decisions
about what parts of the video need the most bitrate.</p>
<p>How do we fix this?
x264 supports what is known as 2-pass encoding.
In 2-pass mode, x264 runs through the video twice,
the first time analyzing it
to determine where to place keyframes
and which sections of video need the most bitrate,
and the second time performing the actual encode.
2-pass mode is highly recommended
if you need to target a certain bitrate.</p>
<p>Here’s how we would run our first pass:</p>
<pre><code>vspipe --y4m myvideo.vpy - | x264 --demuxer y4m --preset veryfast --pass 1 --bitrate 5222 -o x264output.mkv -
</code></pre>
<p>This creates a stats file in our current directory,
which x264 will use in the second pass:</p>
<pre><code>vspipe --y4m myvideo.vpy - | x264 --demuxer y4m --preset veryfast --pass 2 --bitrate 5222 -o x264output.mkv -
</code></pre>
<p>You’ll notice all we had to change was <code>--pass 1</code> to <code>--pass 2</code>. Simple!</p>
<p>Although x264 will automatically use faster settings for the first pass,
it should be no surprise
that 2-pass encoding is slower than 1-pass encoding.
Therefore, there are still certain use cases
where 1-pass, bitrate-targeted video
is a good fit, such as streaming.</p>
<h2><a class="header" href="#recap" id="recap">Recap</a></h2>
<p>We covered the basics of how to encode in x264,
including speed presets, tunings, and three different encoding modes.</p>
<p>Here is a summary of when to use each encoding mode:</p>
<ul>
<li>1-pass Constant Quality (CRF):
<ul>
<li>Good for: General-purpose encoding</li>
<li>Bad for: Streaming; obtaining a certain file size</li>
</ul>
</li>
<li>1-pass Average Bitrate:
<ul>
<li>Good for: Streaming</li>
<li>Bad for: Everything else</li>
</ul>
</li>
<li>2-pass Average Bitrate:
<ul>
<li>Good for: Obtaining a certain file size</li>
<li>Bad for: Streaming</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#advanced-configuration" id="advanced-configuration">Advanced Configuration</a></h2>
<p>Coming Soon</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Source: <a href="http://www.mpeg.org/MPEG/DVD/Book_A/Specs.html">http://www.mpeg.org/MPEG/DVD/Book_A/Specs.html</a></p>
</div>
<h1><a class="header" href="#aegisub--other-tools" id="aegisub--other-tools">Aegisub &amp; Other Tools</a></h1>
<h2><a class="header" href="#tools" id="tools">Tools</a></h2>
<p>The first thing you’ll need to do is make sure your tools are in order.
Typesetters will need more tools than most other roles in fansubbing and
they need to be configured properly.</p>
<p>Here is a list of tools you will want to download:</p>
<ul>
<li><a href="http://www.aegisub.org">Aegisub</a>
<ul>
<li>It is <strong>highly</strong> recommended to use <a href="https://thevacuumof.space/builds/">CoffeeFlux’s builds</a><sup class="footnote-reference"><a href="#1">1</a></sup> which
include Dependency Control and several <em>critical</em>
fixes to Aegisub that have not been merged into the official
application.</li>
</ul>
</li>
<li>A font manager
<ul>
<li>Not all font managers are equal. Choose the one that works the
best for you. Some important features might include:
<ul>
<li>Performance with large font libraries.</li>
<li>Add fonts from folders, not just installed fonts.</li>
<li>Activate fonts for use without installing.</li>
<li>Organize fonts in a meaningful way.</li>
<li>Works on your OS.</li>
</ul>
</li>
<li>Free Options
<ul>
<li><a href="http://www.xiles.net">Nexusfont</a></li>
<li><a href="https://fontba.se/">FontBase</a></li>
</ul>
</li>
<li>Paid (Note: can be found free on <em>certain websites)</em>
<ul>
<li><a href="https://www.high-logic.com/font-manager/maintype">MainType</a></li>
<li><a href="https://www.extensis.com/products/font-management/suitcase-fusion/">Suitcase Fusion</a></li>
</ul>
</li>
</ul>
</li>
<li>Software for motion-tracking
<ul>
<li><a href="https://www.imagineersystems.com/products/mocha-pro/">Mocha Pro standalone app</a>
<ul>
<li>Look for it on <em>certain websites</em>.</li>
<li>On Windows, Mocha <strong>requires</strong> <a href="https://support.apple.com/kb/DL837?locale=en_US">Quicktime</a> to be installed.
More information can be found <a href="https://borisfx.com/faq/quicktime-on-windows/">here</a>.</li>
</ul>
</li>
</ul>
</li>
<li><a href="https://artifacts.videolan.org/x264/">x264 binary</a><sup class="footnote-reference"><a href="#2">2</a></sup>
<ul>
<li>Download the latest binary for your platform. (<code>x264-r3015-4c2aafd</code> at the time of this edit.)</li>
</ul>
</li>
<li><a href="https://www.adobe.com/creativecloud.html">Adobe Photoshop and Illustrator</a>
<ul>
<li>Look for it on <em>certain websites</em>.</li>
<li>Alternatively, free software like
<a href="https://www.gimp.org">Gimp</a> and
<a href="https://inkscape.org/en/">Inkscape</a> may be used in some
circumstances.</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#configuring-aegisub" id="configuring-aegisub">Configuring Aegisub</a></h2>
<p>NOTE: the following assumes you have installed the recommended build
mentioned above.</p>
<p>For now, just change your settings to reflect the following.
If you’ve made any changes previously for another fansub role,
be careful not to overwrite those.
When in doubt, ask someone with Aegisub experience.
Settings can be accessed via <em>View &gt; Options</em>
or with the hotkey <em>Alt + O</em>.</p>
<p><img src="typesetting/images/preferences-1.png" alt="Aegisub 8975-master-8d77da3 preferences 1" /></p>
<p><img src="typesetting/images/preferences-2.png" alt="Aegisub 8975-master-8d77da3 preferences 2" /></p>
<p><img src="typesetting/images/preferences-3.png" alt="Aegisub 8975-master-8d77da3 preferences 3" />
<em>Aegisub 8975-master-8d77da3 preferences</em></p>
<p>Under <em>File &gt; Properties</em>,
there is an additional option for the <em>YCbCr Matrix</em> of the script.
This option will set the color space of the script,
and you will most likely be working with TV.709,
or BT.709.
If you are subtitling with a video present
(using <em>Video &gt; Open Video...</em>),
this option as well as the script resolution
will automatically be set to match the video source.</p>
<p><img src="typesetting/images/script_properties-1.png" alt="" />
<em>Aegisub 8975-master-8d77da3 script properties</em></p>
<p>For most cases with modern fansubbing,
the BT.709 color space will be used
as opposed to the legacy BT.601 color space.
If you want a more in-depth explanation of color matrices
and how these two are different,
visit <a href="typesetting/../archived-websites/bt601-vs-bt709.html">Maxime Lebled’s blog</a>,
but the gist of it is this:
BT.601 is for Standard Definition video
and BT.709 is for High Definition video<sup class="footnote-reference"><a href="#3">3</a></sup>.</p>
<p>Manually setting the script to BT.601 could
<strong>irreversibly ruin the colors of any typesetting,
dialogue,
or kfx already in the script</strong>.
Even worse,
some video renderers will read this setting from the muxed subtitles
and render the video to match it.</p>
<p>If you are working on a DVD
or something in Standard Definition,
you can change this to BT.601 manually in <em>File &gt; Script Properties</em>.
However, not all Standard Definition video will be BT.601,
so when in doubt,
ask the encoder or check the source’s
<a href="https://mediaarea.net/en/MediaInfo">MediaInfo</a> if they are not available.</p>
<h3><a class="header" href="#the-subtitles-provider" id="the-subtitles-provider">The “Subtitles Provider”</a></h3>
<p>The recommended build of Aegisub comes pre-equipped with libass,
so no manual settings change is needed.
The following is a brief history of subtitle renderers.</p>
<p>Just a few years ago,
there was a pretty clear consensus on which
subtitle renderer to use for anime and softsubs.
These days, not so much.
It used to be that <a href="https://sourceforge.net/projects/guliverkli/files/VSFilter/">VSFilter</a>
was the only supported renderer by most fansub groups.
VSFilter, being the first of its kind,
is considered the original subtitle renderer.
However, it was eventually replaced with <a href="https://forum.doom9.org/showthread.php?t=168282">xy-VSFilter</a>,
and then later replaced with <a href="https://forum.doom9.org/showthread.php?t=168282">xySubFilter</a>
because VSFilter and xy-vsfilter were not performing as well
with the resource requirements of newer subtitles.
However, VSFilter,
and its derivatives xy-vsfilter and xySubFilter,
only support Windows operating systems.
They have often been used in codec packs<sup class="footnote-reference"><a href="#4">4</a></sup> for
players we don’t recommend,
such as <a href="https://mpc-hc.org/">MPC-HC</a>.</p>
<p>By 2015, however,
xySubFilter development had come to a halt and since then,
<a href="https://github.com/libass/libass">libass</a> has made many improvements
both in speed and compatibility with advanced subtitling
in part due to contributions from members of the fansub community.
At the end of the day,
which renderer you choose is up to you,
but we recommend libass.
It is maintained,
cross-platform,
able to handle most typesetting,
and has been integrated into many commercial
and open-source software products.
Libass is used in the cross-platform player
<a href="https://mpv.io/">mpv</a>,
that we recommend for all anime-viewing purposes.</p>
<h3><a class="header" href="#hotkeys" id="hotkeys">Hotkeys</a></h3>
<p>As you develop your skills more
and begin to integrate automation scripts into your workflow,
you will probably want to consider adding
hotkeys to cut down on time navigating menus.
These can be accessed via
<em>Interface &gt; Hotkeys</em>
in Aegisub’s Options menu.
We’ll let you decide on those yourself, however,
and move on for now.</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>A long outstanding bug has made the recent versions of Aegisub unstable.
The latest stable version as of writing this, r8903+1,
can be found on <a href="https://www.goodjobmedia.com/fansubbing/">GoodJobMedia’s website</a>.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>32-bit builds on Windows may be more stable.</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>For further reading on this,
visit the Wikipedia pages for <a href="https://en.wikipedia.org/wiki/Standard-definition_television">Standard Definition</a> video,
<a href="https://en.wikipedia.org/wiki/High-definition_video">High Definition</a> video,
and the <a href="https://en.wikipedia.org/wiki/Rec._601">BT.601</a> and <a href="https://en.wikipedia.org/wiki/Rec._709">BT.709</a> color spaces.</p>
</div>
<div class="footnote-definition" id="4"><sup class="footnote-definition-label">4</sup>
<p>With the development of <a href="https://mpv.io/">mpv</a>,
codec packs and player add-ons are no longer required.</p>
</div>
<p>Note: this is an archived version of a webpage,
the full webpage may still be up at the link at the bottom.</p>
<hr />
<p>We’re in 2016 yet our computers insist on shipping with atrocious
default video decoding settings. The first cardinal sin, which is
immediately noticeable, is having a 16-235 range instead of 0-255.
Blacks become grey, and pure white is slightly dim.</p>
<p>...</p>
<p>BT.601
is a standard from 1982 which, among other things, defines how RGB color
primaries get turned into the YCbCr channels used by modern
codecs. BT.709
is a standard from 1990 which does the same thing, but the transfer
coefficients are slightly different. And unlike its older counterpart,
it was not thought of with standard definition in mind, but HD television.</p>
<p>Here’s the problem: a lot of consumer-grade software is either not aware
of the difference between the two, or encodes and/or decodes with the
wrong one. There are also assumptions being made; video tends to be
decoded as 709 if it’s above 720 pixels in height, and 601 if below,
<strong>regardless</strong> of the coefficients it was actually originally encoded with.</p>
<p>...</p>
<p>Anyway, generally speaking:</p>
<ul>
<li><em>Red too orange, green too dark?</em> 601 is being decoded as 709.</li>
<li><em>Red too dark, green too yellowish?</em> 709 is being decoded as 601.</li>
</ul>
<p>What can you do about this?</p>
<p>Figure a way for your video pipeline to properly handle both.</p>
<p>Adobe Media Encoder may not be as bitrate-efficient as your
run-of-the-mill x264+GUI combo, but it does all the righti things and
writes all the right metadata to make a fully compliant file. And I
would argue in this day and age, when you’re encoding to send a file to
YouTube, it doesn’t really matter if you’re picking a less
bitrate-efficient encoder because if you care about quality to begin
with, you’ll be sending a file as high bitrate as possible (probably
reaching above 0.4 bits per pixel).</p>
<p><em>In fact, I just double-checked as I was writing this post: Adobe Media
Encoder is the only software I know of which actually encodes video with
its proper YCbCR transfer coefficients</em>.</p>
<p>...</p>
<hr />
<p>Lebled, M. (2016, August 02). BT.601 vs BT.709. Retrieved from http://blog.maxofs2d.net/post/148346073513/bt601-vs-bt709</p>
<h1><a class="header" href="#impressum" id="impressum">Impressum</a></h1>
<p><em>Required by § 5 TMG (Germany)</em> <a href="https://www.gesetze-im-internet.de/tmg/__5.html"><i class="fa fa-gavel"></i></a></p>
<p>Roland Netzsch</p>
<p>Feichtetstraße 27</p>
<p>82343 Pöcking</p>
<h2><a class="header" href="#contact-information" id="contact-information">Contact Information</a></h2>
<p>Telephone: +49 8157 5971694</p>
<p>E-Mail: stuxcrystal@encode.moe <a href="mailto:stuxcrystal@encode.moe"><i class="fa fa-envelope"></i></a></p>
<p>Internet address: https://guide.encode.moe/</p>
<h2><a class="header" href="#disclaimer" id="disclaimer">Disclaimer</a></h2>
<h3><a class="header" href="#accountability-for-content" id="accountability-for-content">Accountability for content</a></h3>
<p>The contents of our pages have been created with the utmost care.
However, we cannot guarantee the contents’ accuracy, completeness or
topicality.
According to statutory provisions, we are furthermore responsible for our own
content on these web pages.
In this matter, please note that we are not obliged to monitor the transmitted
or saved information of third parties, or investigate circumstances pointing to
illegal activity.
Our obligations to remove or block the use of information under generally
applicable laws remain unaffected by this as per
§§ 8 to 10 of the Telemedia Act (TMG).</p>
<h3><a class="header" href="#accountability-for-links" id="accountability-for-links">Accountability for links</a></h3>
<p>Responsibility for the content of external links (to web pages of third parties)
lies solely with the operators of the linked pages.
No violations were evident to us at the time of linking.
Should any legal infringement become known to us,
we will remove the respective link immediately.</p>
<h3><a class="header" href="#copyright" id="copyright">Copyright</a></h3>
<p>Our web pages and their contents are subject to German copyright law.
Unless expressly permitted by law, every form of utilizing, reproducing or
processing works subject to copyright protection on our web pages requires the
prior consent of the respective owner of the rights.
The materials from these pages are copyrighted and any unauthorized use may
violate copyright laws.</p>
<p>Source: <a href="https://www.translate-24h.de/">Englisch-Übersetzungsdienst translate-24h</a> <em>(modified)</em></p>
<h1><a class="header" href="#privacy-policy" id="privacy-policy">Privacy Policy</a></h1>
<ol>
<li>
<p>This page does not store any of your data by itself.</p>
</li>
<li>
<p>We are using the services of CloudFlare.</p>
<p><a href="https://www.cloudflare.com/privacypolicy/">CloudFlare Privacy Policy</a></p>
<p>This service uses cookies to authenticate you so you won’t be shown
captchas on your visit.
We are using extensions provided by CloudFlare that detects your browser
version and injects additional content if you are using an outdated browser.</p>
</li>
<li>
<p>This project is hosted on GitHub.</p>
<p><a href="https://help.github.com/en/articles/github-privacy-statement">GitHub Privacy Policy</a></p>
<p>GitHub may store information about your visit in the form of log files.
Read the privacy policy of GitHub for further information.</p>
</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
